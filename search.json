[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Max Lorsignol",
    "section": "",
    "text": "Hi, my name is Max Lorsignol.\nI am currently pursuing a Master’s degree in Geomatics for Environmental Management at UBC. What is geomatics, you ask? In a nutshell, geomatics is the science and technology of collecting, analyzing, managing, and visualizing spatial data. Ever since I was a child, I’ve been fascinated by the maps my father used for his mountaineering adventures. Today, I’m leveraging that lifelong love for mapping and applying it with modern tools and standards to be a part of the conservation and environmental efforts of the 21st century.\nWhen I’m not crafting digital maps or spatial analyses, you’ll find me outdoors, navigating the rugged mountains of the west coast with the same passion for exploration. My goal is to combine the skills I’ve developed through my education and personal experiences to address pressing issues related to environmental management, natural resources, climate change, and land use management. I hope to make a meaningful difference in creating sustainable solutions for the challenges our world faces today.\n\n\n\nGIF of a forest plot with Individual Trees Segmented using li2012, colorized by TreeID"
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Resume",
    "section": "",
    "text": "Education\n\nB.Sc Earth and Environmental Science UBC Okanagan\nMasters in Geomatics for Environmental Management UBC Vancouver\n\n\n\nProfessional Experience\n\nLead Field Technician at AquaCoustic Remote Technologies Inc\n\nSetting up, testing and deploying robotic platforms including crawlers and pontoons of different sizes, ROVs, and submersibles to collect video, sonar, laser and additional sensor data\nInspections, cleaning and maintenance of robotic platforms, fiber optic winches, vehicles and other warehouse equipment\nHands-on troubleshooting, assembly and repair of equipment inhouse and in the field\nKeeping accurate daily logs and other detailed inspections sheets and paperwork\nUtility vehicle interior designing, building and maintenance, including office space, electronics and winches\nNightwork and all-weather type field work, working independently, within a team capacity and with other contractors\nProcessing sonar, laser and video data using custom and sourced software\n\nVisitors Service Attendant for Banff National Park\n\nPublic outreach - Providing orientation, information and suggestions to visitors in Banff National Park\nEnforcing Park closures, restrictions and regulations with compliance and ticketing\nEmployee uniform and accessories inventory, distribution and organizing\nUpdating Dispatch and Resource Conservation on wildlife encounters, fires and other public safety concerns\nCustomer service - Handling cash and credit POS transactions in any weather situation quickly and effectively\n\n\n\n\nPublications\nReports:\n\nFCOR 599 OECM Land Designation in Howe Sound Biosphere.\n\n\n\nOther Experiences\nVolunteering:\n\nStanley Park Ecology DIRT (Dedicated Invasive Removal Team)\nAME Roundup\nChuckleBerry Organic Farm"
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "Peer-reviewed publications:\nDespite listing them on your resume, it may also be pertinent to create a seperate tab for publications and reports. As your career progresses, this list may become quite long, so be sure to organize things. You may want to point to ‘most recent’ publications, or categorize things by project/topic."
  },
  {
    "objectID": "content_development.html",
    "href": "content_development.html",
    "title": "Content & Deliverables",
    "section": "",
    "text": "Project Deliverable 1\nThis is a sample page where you can archive project deliverables."
  },
  {
    "objectID": "content.html",
    "href": "content.html",
    "title": "Content & Deliverables",
    "section": "",
    "text": "This project focuses on identifying and designating Other Effective Area-Based Conservation Measures (OECMs) within the Átl’ḵa7tsem/Howe Sound Biosphere Region. Using geospatial data from BC Parcel Data, CPCAD, and ecological sensitivity datasets, the study aims to assess parcels of land for their potential to support biodiversity conservation while meeting the criteria for OECM designation. The methodology involves creating a “mesh sensitivity layer” that integrates critical habitat data, forest age (&gt;200 years), and proximity to streams, combined with parcel attributes like ownership and zoning.\nSpatial analysis, guided by a decision support tool, evaluates parcels based on ecological sensitivity, proximity to existing conservation areas, and governance capacity. The project also examines differences across regional districts (Metro Vancouver, Sunshine Coast, Squamish Lillooet), aiming to prioritize parcels that are unprotected but ecologically significant. Expected results include a map of high-priority OECM candidates, helping to enhance biodiversity conservation while contributing to Canada’s broader conservation goals under the CBD’s Aichi Targets.\n\n\nEach hexagon has a value based on the ecological sensitivity, critical habitat, forest age, proximity to streams and slope steepness. Darker values represent more sensitive areas and are used to select parcels of land from the BC Parcel Fabric layer in order to locate pieces of land that could be considered as OECM.\n\nmain_map\n\n\n\n\n\n\n\n\nUsing the suitability mesh layer in conjunction with the BC Fabric Parcel layer, I can select parcels that intersect with areas of the mesh where the total_weight is above a specified threshold. In the example below, a conservative value of 2.6, well above the mean, is used to select parcels. The resulting parcels can be seen below. These parcels are then categorized based on their zoning, OCP and municipality. A final list of potential OECMs are given to Howe Sound Biosphere Region Initiative Society (HSBRIS) for them to follow up with those selected parcels and organize an approach to start the land designation change."
  },
  {
    "objectID": "content.html#leaflet",
    "href": "content.html#leaflet",
    "title": "Content & Deliverables",
    "section": "",
    "text": "Sample leaflet - for detailed leaflet instructions, visit the FCOR 599 workshop archive page here.\n\nmain_map"
  },
  {
    "objectID": "content.html#code-snippets",
    "href": "content.html#code-snippets",
    "title": "Content & Deliverables",
    "section": "",
    "text": "Sample code snippet. Notice that you can provide a toggle to switch between coding languages - this is referred to as a ‘tabset’ in quarto. It is good practice to try and convert your R code to python, and vice-versa to demonstrate coding proficiency. For example, let’s showcase a function for calculating NDVI in R and Python.\n\nRPython\n\n\ncalc_ndvi &lt;- function(nir, red){ ndvi &lt;- (nir-red)/(nir+red) return(ndvi) }\n\n\ndef calc_ndvi(nir, red): \n  ndvi = (nir.astype(float)-red.astype(float))/(nir.astype(float)+red.astype(float))\n  return(ndvi)"
  },
  {
    "objectID": "content.html#external-links",
    "href": "content.html#external-links",
    "title": "Content & Deliverables",
    "section": "",
    "text": "We can also provide a frame linking to external websites. For example, here is a link to a Google Earth Engine application I developed. The full-screen GEE application is available here in case you’re interested.\n(To use the GEE tool, navigate to any city you’d like, hit apply filters, and click anywhere on the map to retrieve a time-series of Landsat surface temperature observations for that point. Areas where the maximum temp exceeded 35 degrees Celsius in your date-range are highlighted in red.)"
  },
  {
    "objectID": "content.html#oecm-land-suitability-mesh-layer",
    "href": "content.html#oecm-land-suitability-mesh-layer",
    "title": "Content & Deliverables",
    "section": "",
    "text": "Each hexagon has a value based on the ecological sensitivity, critical habitat, forest age, proximity to streams and slope steepness. Darker values represent more sensitive areas and are used to select parcels of land from the BC Parcel Fabric layer in order to locate pieces of land that could be considered as OECM.\n\nmain_map"
  },
  {
    "objectID": "content.html#bc-fabric-parcel-selection",
    "href": "content.html#bc-fabric-parcel-selection",
    "title": "Content & Deliverables",
    "section": "",
    "text": "Using the suitability mesh layer in conjunction with the BC Fabric Parcel layer, I can select parcels that intersect with areas of the mesh where the total_weight is above a specified threshold. In the example below, a conservative value of 2.6, well above the mean, is used to select parcels. The resulting parcels can be seen below. These parcels are then categorized based on their zoning, OCP and municipality. A final list of potential OECMs are given to Howe Sound Biosphere Region Initiative Society (HSBRIS) for them to follow up with those selected parcels and organize an approach to start the land designation change."
  },
  {
    "objectID": "backup_content.html",
    "href": "backup_content.html",
    "title": "Content & Deliverables",
    "section": "",
    "text": "Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "backup_content.html#quarto",
    "href": "backup_content.html#quarto",
    "title": "Content & Deliverables",
    "section": "",
    "text": "Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "backup_content.html#running-code",
    "href": "backup_content.html#running-code",
    "title": "Content & Deliverables",
    "section": "Running Code",
    "text": "Running Code\nWhen you click the Render button a document will be generated that includes both content and the output of embedded code. You can embed code like this:\n\n1 + 1\n\n[1] 2\n\n\nYou can add options to executable code like this\n\n\n[1] 4\n\n\nThe echo: false option disables the printing of code (only output is displayed)."
  },
  {
    "objectID": "backup_content.html#oecm-land-suitability-mesh-layer",
    "href": "backup_content.html#oecm-land-suitability-mesh-layer",
    "title": "Content & Deliverables",
    "section": "OECM Land Suitability Mesh Layer",
    "text": "OECM Land Suitability Mesh Layer\nEach hexagon has a value based on the ecological sensitivity, critical habitat, forest age, proximity to streams and slope steepness. Darker values represent more sensitive areas and are used to select parcels of land from the BC Parcel Fabric layer in order to locate pieces of land that could be considered as OECM.\n\nmain_map"
  },
  {
    "objectID": "backup_content.html#code-snippets",
    "href": "backup_content.html#code-snippets",
    "title": "Content & Deliverables",
    "section": "Code Snippets",
    "text": "Code Snippets\nSample code snippet. Notice that you can provide a toggle to switch between coding languages - this is referred to as a ‘tabset’ in quarto. It is good practice to try and convert your R code to python, and vice-versa to demonstrate coding proficiency. For example, let’s showcase a function for calculating NDVI in R and Python.\n\nRPython\n\n\ncalc_ndvi &lt;- function(nir, red){ ndvi &lt;- (nir-red)/(nir+red) return(ndvi) }\n\n\ndef calc_ndvi(nir, red): \n  ndvi = (nir.astype(float)-red.astype(float))/(nir.astype(float)+red.astype(float))\n  return(ndvi)"
  },
  {
    "objectID": "backup_content.html#external-links",
    "href": "backup_content.html#external-links",
    "title": "Content & Deliverables",
    "section": "External links",
    "text": "External links\nWe can also provide a frame linking to external websites. For example, here is a link to a Google Earth Engine application I developed. The full-screen GEE application is available here in case you’re interested.\n(To use the GEE tool, navigate to any city you’d like, hit apply filters, and click anywhere on the map to retrieve a time-series of Landsat surface temperature observations for that point. Areas where the maximum temp exceeded 35 degrees Celsius in your date-range are highlighted in red.)"
  },
  {
    "objectID": "portfolio_personal.html",
    "href": "portfolio_personal.html",
    "title": "Personal Projects",
    "section": "",
    "text": "Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "portfolio_personal.html#quarto",
    "href": "portfolio_personal.html#quarto",
    "title": "Personal Projects",
    "section": "",
    "text": "Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "portfolio_personal.html#running-code",
    "href": "portfolio_personal.html#running-code",
    "title": "Personal Projects",
    "section": "Running Code",
    "text": "Running Code\nWhen you click the Render button a document will be generated that includes both content and the output of embedded code. You can embed code like this:\n\n1 + 1\n\n[1] 2\n\n\nYou can add options to executable code like this\n\n\n[1] 4\n\n\nThe echo: false option disables the printing of code (only output is displayed)."
  },
  {
    "objectID": "CV.html",
    "href": "CV.html",
    "title": "CV",
    "section": "",
    "text": "Education\n\nB.Sc Earth and Environmental Science UBC Okanagan\nMasters in Geomatics for Environmental Management UBC Vancouver\n\n\n\nProfessional Experience\n\nLead Field Technician at AquaCoustic Remote Technologies Inc\n\nSetting up, testing and deploying robotic platforms including crawlers and pontoons of different sizes, ROVs, and submersibles to collect video, sonar, laser and additional sensor data\nInspections, cleaning and maintenance of robotic platforms, fiber optic winches, vehicles and other warehouse equipment\nHands-on troubleshooting, assembly and repair of equipment inhouse and in the field\nKeeping accurate daily logs and other detailed inspections sheets and paperwork\nUtility vehicle interior designing, building and maintenance, including office space, electronics and winches\nNightwork and all-weather type field work, working independently, within a team capacity and with other contractors\nProcessing sonar, laser and video data using custom and sourced software\n\nVisitors Service Attendant for Banff National Park\n\nPublic outreach - Providing orientation, information and suggestions to visitors in Banff National Park\nEnforcing Park closures, restrictions and regulations with compliance and ticketing\nEmployee uniform and accessories inventory, distribution and organizing\nUpdating Dispatch and Resource Conservation on wildlife encounters, fires and other public safety concerns\nCustomer service - Handling cash and credit POS transactions in any weather situation quickly and effectively\n\n\n\n\nPublications\nReports:\n\nFCOR 599 OECM Land Designation in Howe Sound Biosphere.\n\n\n\nOther Experiences\nVolunteering:\n\nStanley Park Ecology DIRT (Dedicated Invasive Removal Team)\nAME Roundup\nChuckleBerry Organic Farm"
  },
  {
    "objectID": "portfolio_geospatial.html",
    "href": "portfolio_geospatial.html",
    "title": "Geospatial Analysis",
    "section": "",
    "text": "This project focuses on identifying and designating Other Effective Area-Based Conservation Measures (OECMs) within the Átl’ḵa7tsem/Howe Sound Biosphere Region. Using geospatial data from BC Parcel Data, CPCAD, and ecological sensitivity datasets, the study aims to assess parcels of land for their potential to support biodiversity conservation while meeting the criteria for OECM designation. The methodology involves creating a “mesh sensitivity layer” that integrates critical habitat data, forest age (/&gt;200 years), and proximity to streams, combined with parcel attributes like ownership and zoning.\nSpatial analysis, guided by a decision support tool, evaluates parcels based on ecological sensitivity, proximity to existing conservation areas, and governance capacity. The project also examines differences across regional districts (Metro Vancouver, Sunshine Coast, Squamish Lillooet), aiming to prioritize parcels that are unprotected but ecologically significant. Expected results include a map of high-priority OECM candidates, helping to enhance biodiversity conservation while contributing to Canada’s broader conservation goals under the CBD’s Aichi Targets.\nEach hexagon has a value based on the ecological sensitivity, critical habitat, forest age, proximity to streams and slope steepness. Darker values represent more sensitive areas and are used to select parcels of land from the BC Parcel Fabric layer in order to locate pieces of land that could be considered as OECM.\n\n\nUsing the suitability mesh layer in conjunction with the BC Fabric Parcel layer, I can select parcels that intersect with areas of the mesh where the total_weight is above a specified threshold. In the example below, a conservative value of 2.6, well above the mean, is used to select parcels. The resulting parcels can be seen below. These parcels are then categorized based on their zoning, OCP and municipality. A final list of potential OECMs are given to Howe Sound Biosphere Region Initiative Society (HSBRIS) for them to follow up with those selected parcels and organize an approach to start the land designation change."
  },
  {
    "objectID": "portfolio_geospatial.html#oecm-land-suitability-mesh-layer",
    "href": "portfolio_geospatial.html#oecm-land-suitability-mesh-layer",
    "title": "Geospatial Analysis",
    "section": "",
    "text": "Each hexagon has a value based on the ecological sensitivity, critical habitat, forest age, proximity to streams and slope steepness. Darker values represent more sensitive areas and are used to select parcels of land from the BC Parcel Fabric layer in order to locate pieces of land that could be considered as OECM."
  },
  {
    "objectID": "portfolio_geospatial.html#bc-fabric-parcel-selection",
    "href": "portfolio_geospatial.html#bc-fabric-parcel-selection",
    "title": "Geospatial Analysis",
    "section": "",
    "text": "Using the suitability mesh layer in conjunction with the BC Fabric Parcel layer, I can select parcels that intersect with areas of the mesh where the total_weight is above a specified threshold. In the example below, a conservative value of 2.6, well above the mean, is used to select parcels. The resulting parcels can be seen below. These parcels are then categorized based on their zoning, OCP and municipality. A final list of potential OECMs are given to Howe Sound Biosphere Region Initiative Society (HSBRIS) for them to follow up with those selected parcels and organize an approach to start the land designation change."
  },
  {
    "objectID": "portfolio_remote_sensing.html",
    "href": "portfolio_remote_sensing.html",
    "title": "Remote Sensing",
    "section": "",
    "text": "During my free time and from my Masters of Geomatics for Environmental Management (MGEM) at UBC, I have learned a range of remote sensing techniques and skills.\nBelow are some of those projects, current and past."
  },
  {
    "objectID": "portfolio_remote_sensing.html#quarto",
    "href": "portfolio_remote_sensing.html#quarto",
    "title": "Remote Sensing",
    "section": "",
    "text": "Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "portfolio_remote_sensing.html#running-code",
    "href": "portfolio_remote_sensing.html#running-code",
    "title": "Remote Sensing",
    "section": "Running Code",
    "text": "Running Code\nWhen you click the Render button a document will be generated that includes both content and the output of embedded code. You can embed code like this:\n\n1 + 1\n\n[1] 2\n\n\nYou can add options to executable code like this\n\n\n[1] 4\n\n\nThe echo: false option disables the printing of code (only output is displayed)."
  },
  {
    "objectID": "portfolio_landscape_ecology.html",
    "href": "portfolio_landscape_ecology.html",
    "title": "Landscape Ecology",
    "section": "",
    "text": "This page is currently a work in progress.\n\n\n  layer       crs units   class n_classes OK\n1     1 projected     m integer        17  ✔\n\n\n\n\n# A tibble: 10 × 4\n      ID Class                       Color   Description                        \n   &lt;dbl&gt; &lt;chr&gt;                       &lt;chr&gt;   &lt;chr&gt;                              \n 1    11 Open Water                  #5475A8 Areas of open water, generally wit…\n 2    21 Developed, Open Space       #E8D1D1 Areas with a mixture of some const…\n 3    22 Developed, Low Intensity    #E29E8C Areas with a mixture of constructe…\n 4    23 Developed, Medium Intensity #ff0000 Areas with a mixture of constructe…\n 5    41 Deciduous Forest            #85C77E Areas dominated by trees generally…\n 6    42 Evergreen Forest            #38814E Areas dominated by trees generally…\n 7    43 Mixed Forest                #D4E7B0 Areas dominated by trees generally…\n 8    52 Shrub/Scrub                 #DCCA8F Areas dominated by shrubs; less th…\n 9    82 Cultivated Crops            #CA9146 Areas used for the production of a…\n10    90 Woody Wetlands              #C8E6F8 Areas where forest or shrubland ve…"
  },
  {
    "objectID": "portfolio_landscape_ecology.html#quarto",
    "href": "portfolio_landscape_ecology.html#quarto",
    "title": "Landscape Ecology",
    "section": "",
    "text": "Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "portfolio_landscape_ecology.html#running-code",
    "href": "portfolio_landscape_ecology.html#running-code",
    "title": "Landscape Ecology",
    "section": "Running Code",
    "text": "Running Code\nWhen you click the Render button a document will be generated that includes both content and the output of embedded code. You can embed code like this:\n\n1 + 1\n\n[1] 2\n\n\nYou can add options to executable code like this\n\n\n[1] 4\n\n\nThe echo: false option disables the printing of code (only output is displayed)."
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Contact",
    "section": "",
    "text": "I am always eager to connect! If you have any questions, opportunities or just want to say hi, feel free to reach out through any of the channels below:\nemail: maxlorsignol@gmail.com\nphone: 780-340-6128\nLinkedIn: Max Lorsignol\nGitHub: maxlorsignol"
  },
  {
    "objectID": "contact.html#quarto",
    "href": "contact.html#quarto",
    "title": "Contact",
    "section": "",
    "text": "Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "contact.html#running-code",
    "href": "contact.html#running-code",
    "title": "Contact",
    "section": "Running Code",
    "text": "Running Code\nWhen you click the Render button a document will be generated that includes both content and the output of embedded code. You can embed code like this:\n\n1 + 1\n\n[1] 2\n\n\nYou can add options to executable code like this\n\n\n[1] 4\n\n\nThe echo: false option disables the printing of code (only output is displayed)."
  },
  {
    "objectID": "portfolio_remote_sensing.html#examples-of-remote-sensing-experience",
    "href": "portfolio_remote_sensing.html#examples-of-remote-sensing-experience",
    "title": "Remote Sensing",
    "section": "",
    "text": "Below are some projects in remote sensing techniques that I have become familiar with during my Masters program at UBC."
  },
  {
    "objectID": "portfolio_remote_sensing.html#land-classification",
    "href": "portfolio_remote_sensing.html#land-classification",
    "title": "Remote Sensing",
    "section": "Land Classification",
    "text": "Land Classification\nThe Remote Sensing course in MGEM taught me how to produce landscape classifications using a variety of supervised and unsupervised classification processes. Fascinated by different classification processes and the maps it produced, I extended this skill to produce a number of classified maps of various landscapes where Landsat and Copernicus satellite data was available. Generally my focus was with the Howe Sound Biosphere as it was part of my capstone project at UBC and is where I spend my time hiking, skiing and climbing. Below are some of the results of recent pixel classifications.\nBelow is a slide-by-side comparison of two different classification processes of the Howe Sound. The left side is classified under a supervised process using hand-delineated polygons, 20/80 training to validation, with custom R code (a lot of work). The right shows a classification using the ArcGIS Pro deeplearning land cover classification (almost instantaneous).\n\n\n\n\n\n\nThe idea here is to visualize the difference and address the accuracies between the two classification models. How do you think they compare?"
  },
  {
    "objectID": "portfolio_remote_sensing.html#time-series-analysis",
    "href": "portfolio_remote_sensing.html#time-series-analysis",
    "title": "Remote Sensing",
    "section": "Time Series Analysis",
    "text": "Time Series Analysis\nIn this lab, I analyzed time series data in R by first working with MODIS NDVI data. I applied the BFAST algorithm to detect abrupt changes in vegetation—like logging events—that cause rapid declines in NDVI values. I then examined a 33‐year series of land cover maps from Landsat imagery, reclassifying the data into forested and non-forested areas to evaluate trends in forest loss and gain. Throughout the lab, I developed R code for data extraction, time series creation, and visualization, showcasing my practical skills in spatial and temporal analysis with packages such as terra, sf, and bfast."
  },
  {
    "objectID": "portfolio_remote_sensing.html#burn-severity-analysis",
    "href": "portfolio_remote_sensing.html#burn-severity-analysis",
    "title": "Remote Sensing",
    "section": "Burn Severity Analysis",
    "text": "Burn Severity Analysis"
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "CV",
    "section": "",
    "text": "B.Sc Earth and Environmental Science UBC Okanagan (2024-2025)\nMasters in Geomatics for Environmental Management UBC Vancouver (2016-2019)"
  },
  {
    "objectID": "cv.html#lead-field-technician-at-aquacoustic-remote-technologies-inc-2021-2024",
    "href": "cv.html#lead-field-technician-at-aquacoustic-remote-technologies-inc-2021-2024",
    "title": "CV",
    "section": "Lead Field Technician at AquaCoustic Remote Technologies Inc 2021-2024",
    "text": "Lead Field Technician at AquaCoustic Remote Technologies Inc 2021-2024\n\nSetting up, testing and deploying robotic platforms including crawlers and pontoons of different sizes, ROVs, and submersibles to collect video, sonar, laser and additional sensor data\nInspections, cleaning and maintenance of robotic platforms, fiber optic winches, vehicles and other warehouse equipment\nHands-on troubleshooting, assembly and repair of equipment inhouse and in the field\nKeeping accurate daily logs and other detailed inspections sheets and paperwork\nUtility vehicle interior designing, building and maintenance, including office space, electronics and winches\nNightwork and all-weather type field work, working independently, within a team capacity and with other contractors\nProcessing sonar, laser and video data using custom and sourced software"
  },
  {
    "objectID": "cv.html#visitors-service-attendant-for-banff-national-park",
    "href": "cv.html#visitors-service-attendant-for-banff-national-park",
    "title": "CV",
    "section": "Visitors Service Attendant for Banff National Park",
    "text": "Visitors Service Attendant for Banff National Park\n(Summers 2018 and 2019)\n\nPublic outreach: Providing orientation, information and suggestions to visitors in Banff National Park\nEnforcing Park closures, restrictions and regulations with compliance and ticketing\nEmployee uniform and accessories inventory, distribution and organizing\nUpdating Dispatch and Resource Conservation on wildlife encounters, fires and other public safety concerns\nCustomer service - Handling cash and credit POS transactions in any weather situation quickly and effectively"
  },
  {
    "objectID": "cv.html#lead-field-technician-at-aquacoustic-remote-technologies-inc-2020-2024",
    "href": "cv.html#lead-field-technician-at-aquacoustic-remote-technologies-inc-2020-2024",
    "title": "CV",
    "section": "Lead Field Technician at AquaCoustic Remote Technologies Inc (2020-2024)",
    "text": "Lead Field Technician at AquaCoustic Remote Technologies Inc (2020-2024)\n\nSetting up, testing and deploying robotic platforms including crawlers and pontoons of different sizes, ROVs, and submersibles to collect video, sonar, laser and additional sensor data\nInspections, cleaning and maintenance of robotic platforms, fiber optic winches, vehicles and other warehouse equipment\nHands-on troubleshooting, assembly and repair of equipment inhouse and in the field\nKeeping accurate daily logs and other detailed inspections sheets and paperwork\nUtility vehicle interior designing, building and maintenance, including office space, electronics and winches\nNightwork and all-weather type field work, working independently, within a team capacity and with other contractors\nProcessing sonar, laser and video data using custom and sourced software"
  },
  {
    "objectID": "cv.html#co-founder-canopy-campers-van-rentals-2023---present",
    "href": "cv.html#co-founder-canopy-campers-van-rentals-2023---present",
    "title": "CV",
    "section": "Co-Founder Canopy Campers Van Rentals (2023 - Present)",
    "text": "Co-Founder Canopy Campers Van Rentals (2023 - Present)\n\nModelling van interiors using SketchUp for fully off-grid 4-person camper vans\nCustom woodworking and metal working for modular van camping set-ups\nClient interaction & conlfict resolution online and in person, cash and POI transactions\nVehicle maintenance and repairs, regular cleaning, organizing and restocking"
  },
  {
    "objectID": "cv.html#visitors-service-attendant-for-banff-national-park-summers-2018-and-2019",
    "href": "cv.html#visitors-service-attendant-for-banff-national-park-summers-2018-and-2019",
    "title": "CV",
    "section": "Visitors Service Attendant for Banff National Park (Summers 2018 and 2019)",
    "text": "Visitors Service Attendant for Banff National Park (Summers 2018 and 2019)\n\nPublic outreach: Providing orientation, information and suggestions to visitors in Banff National Park\nEnforcing Park closures, restrictions and regulations with compliance and ticketing\nEmployee uniform and accessories inventory, distribution and organizing\nUpdating Dispatch and Resource Conservation on wildlife encounters, fires and other public safety concerns\nCustomer service - Handling cash and credit POS transactions in any weather situation quickly and effectively"
  },
  {
    "objectID": "cv.html#lead-field-technician-at-aquacoustic-remote-technologies-inc",
    "href": "cv.html#lead-field-technician-at-aquacoustic-remote-technologies-inc",
    "title": "CV",
    "section": "Lead Field Technician at AquaCoustic Remote Technologies Inc",
    "text": "Lead Field Technician at AquaCoustic Remote Technologies Inc\n(2020-2024)\n\nSetting up, testing and deploying robotic platforms including crawlers and pontoons of different sizes, ROVs, and submersibles to collect video, sonar, laser and additional sensor data\nInspections, cleaning and maintenance of robotic platforms, fiber optic winches, vehicles and other warehouse equipment\nHands-on troubleshooting, assembly and repair of equipment inhouse and in the field\nKeeping accurate daily logs and other detailed inspections sheets and paperwork\nUtility vehicle interior designing, building and maintenance, including office space, electronics and winches\nNightwork and all-weather type field work, working independently, within a team capacity and with other contractors\nProcessing sonar, laser and video data using custom and sourced software"
  },
  {
    "objectID": "cv.html#co-founder-canopy-campers-van-rentals",
    "href": "cv.html#co-founder-canopy-campers-van-rentals",
    "title": "CV",
    "section": "Co-Founder Canopy Campers Van Rentals",
    "text": "Co-Founder Canopy Campers Van Rentals\n(2023 - Present)\n\nModelling van interiors using SketchUp for fully off-grid 4-person camper vans\nCustom woodworking and metal working for modular van camping set-ups\nClient interaction & conlfict resolution online and in person, cash and POI transactions\nVehicle maintenance and repairs, regular cleaning, organizing and restocking"
  },
  {
    "objectID": "portfolio_landscape_ecology.html#marbled-murrelet-landscape-connectivity-using-grainsofconnectivity",
    "href": "portfolio_landscape_ecology.html#marbled-murrelet-landscape-connectivity-using-grainsofconnectivity",
    "title": "Landscape Ecology",
    "section": "",
    "text": "This page is currently a work in progress.\n\n\n  layer       crs units   class n_classes OK\n1     1 projected     m integer        17  ✔\n\n\n\n\n# A tibble: 10 × 4\n      ID Class                       Color   Description                        \n   &lt;dbl&gt; &lt;chr&gt;                       &lt;chr&gt;   &lt;chr&gt;                              \n 1    11 Open Water                  #5475A8 Areas of open water, generally wit…\n 2    21 Developed, Open Space       #E8D1D1 Areas with a mixture of some const…\n 3    22 Developed, Low Intensity    #E29E8C Areas with a mixture of constructe…\n 4    23 Developed, Medium Intensity #ff0000 Areas with a mixture of constructe…\n 5    41 Deciduous Forest            #85C77E Areas dominated by trees generally…\n 6    42 Evergreen Forest            #38814E Areas dominated by trees generally…\n 7    43 Mixed Forest                #D4E7B0 Areas dominated by trees generally…\n 8    52 Shrub/Scrub                 #DCCA8F Areas dominated by shrubs; less th…\n 9    82 Cultivated Crops            #CA9146 Areas used for the production of a…\n10    90 Woody Wetlands              #C8E6F8 Areas where forest or shrubland ve…"
  },
  {
    "objectID": "portfolio_geospatial.html#work-in-progress",
    "href": "portfolio_geospatial.html#work-in-progress",
    "title": "Geospatial Analysis",
    "section": "",
    "text": "I am currently working on a webmap that allows for the users to dynamically adjust the importance of different layers and immediately see how those changes affect the selection of OECM parcels.\nHere is a Python code snippet.\n\nPython\n\n\n``` (.python, echo=false) import rasterio import numpy as np import geopandas as gpd import matplotlib.pyplot as plt from rasterio.plot import show from rasterio.warp import reproject, Resampling\n\n\nraster_files = { “biological_sensitivity”: r”c:.stuWebApp_habitat_raster.tif”, “watershed”: r”c:.stuWebApp_raster.tif”, “streams”: r”c:.stuWebApp_50m_buffer.tif”, “wetlands”: r”c:.stuWebApp_raster.tif”, “slope”: r”c:.stuWebApp_greaterthan30_raster.tif”, “forest_age”: r”c:.stuWebApp_age_vri.tif” }\n\n\n\nweights = { “biological_sensitivity”: 0.25, “watershed”: 0.15, “streams”: 0.2, “wetlands”: 0.15, “slope”: 0.1, “forest_age”: 0.15 }\n\n\n\nreference_raster = list(raster_files.values())[0]\n\n\n\nwith rasterio.open(reference_raster) as ref_src: ref_transform = ref_src.transform ref_crs = ref_src.crs ref_width = ref_src.width ref_height = ref_src.height ref_dtype = ref_src.dtypes[0]\n\n\n\ndef resample_raster(src_path): with rasterio.open(src_path) as src: data = src.read(1).astype(float) # Read band 1 data[data == src.nodata] = np.nan # Handle NoData values\n    # Create an empty array for resampled data\n    resampled_data = np.empty((ref_height, ref_width), dtype=ref_dtype)\n    \n    # Reproject and resample\n    reproject(\n        source=data,\n        destination=resampled_data,\n        src_transform=src.transform,\n        src_crs=src.crs,\n        dst_transform=ref_transform,\n        dst_crs=ref_crs,\n        resampling=Resampling.bilinear  # Change to nearest for categorical data\n    )\n    \n    return resampled_data\n\n\n\nweighted_sum = np.zeros((ref_height, ref_width), dtype=np.float32) # Initialize empty array\nfor layer, path in raster_files.items(): resampled_raster = resample_raster(path)\nif resampled_raster.shape != (ref_height, ref_width):\n    raise ValueError(f\"Resampled raster '{layer}' has incorrect shape {resampled_raster.shape}\")\n\nweighted_sum += weights[layer] * resampled_raster  # Apply weighted sum\n\n\n\nweighted_sum = (weighted_sum - np.nanmin(weighted_sum)) / (np.nanmax(weighted_sum) - np.nanmin(weighted_sum))\n\n\n\nparcels = gpd.read_file(r”c:.stuWebApp_Parcel_all.shp”)\n\n\n\nthreshold = np.nanpercentile(weighted_sum, 90) # Adjust as needed selected_parcels = parcels.copy() selected_parcels[“suitability”] = selected_parcels.geometry.centroid.apply( lambda pt: weighted_sum[int(pt.y), int(pt.x)] if 0 &lt;= int(pt.y) &lt; ref_height and 0 &lt;= int(pt.x) &lt; ref_width and weighted_sum[int(pt.y), int(pt.x)] &gt;= threshold else np.nan ) selected_parcels = selected_parcels.dropna()\n\n\n\n\n\n\nwith rasterio.open(reference_raster) as src: raster_extent = [src.bounds.left, src.bounds.right, src.bounds.bottom, src.bounds.top]\n\n\n\nparcel_bounds = parcels.total_bounds # [minx, miny, maxx, maxy]\n\n\n\nfig, ax = plt.subplots(figsize=(10, 8))\n\n\n\nshow(weighted_sum, transform=ref_transform, cmap=“viridis”, ax=ax, title=“Weighted Suitability Map”)\n\n\n\nparcels.boundary.plot(ax=ax, edgecolor=“gray”, linewidth=0.5)\n\n\n\nselected_parcels.boundary.plot(ax=ax, edgecolor=“red”, linewidth=1.5, label=“Selected Parcels”)\n\n\n\nax.set_xlim(parcel_bounds[0], parcel_bounds[2]) ax.set_ylim(parcel_bounds[1], parcel_bounds[3])\nplt.legend() plt.show() ```\n\n\n\n\n\n\n\nHoweSoundParcels"
  },
  {
    "objectID": "portfolio_geospatial.html#python",
    "href": "portfolio_geospatial.html#python",
    "title": "Geospatial Analysis",
    "section": "Python",
    "text": "Python\n\n\nPython Code Snippet\nimport rasterio\nimport numpy as np\nimport geopandas as gpd\nimport matplotlib.pyplot as plt\nfrom rasterio.plot import show\nfrom rasterio.warp import reproject, Resampling\n\n# Define input raster files\nraster_files = {\n    \"biological_sensitivity\": r\"C:/Users/lorsigno.stu/Documents/GitHub/E-Portfolio/data/Layers/critical_habitat_raster.tif\",\n    \"watershed\": r\"C:/Users/lorsigno.stu/Documents/GitHub/E-Portfolio/data/Layers/watershed_raster.tif\",\n    \"streams\": r\"C:/Users/lorsigno.stu/Documents/GitHub/E-Portfolio/data/Layers/streams_50m_buffer.tif\",\n    \"wetlands\": r\"C:/Users/lorsigno.stu/Documents/GitHub/E-Portfolio/data/Layers/wetland_raster.tif\",\n    \"slope\": r\"C:/Users/lorsigno.stu/Documents/GitHub/E-Portfolio/data/Layers/slope_greaterthan30_raster.tif\",\n    \"forest_age\": r\"C:/Users/lorsigno.stu/Documents/GitHub/E-Portfolio/data/Layers/forest_age_vri.tif\"\n}\n\n# Define weights\nweights = {\n    \"biological_sensitivity\": 0.25,\n    \"watershed\": 0.15,\n    \"streams\": 0.2,\n    \"wetlands\": 0.15,\n    \"slope\": 0.1,\n    \"forest_age\": 0.15\n}\n\n# Select a reference raster (first raster in the dictionary)\nreference_raster = list(raster_files.values())[0]\n\n# Read reference raster to get target shape, transform, and CRS\nwith rasterio.open(reference_raster) as ref_src:\n    ref_transform = ref_src.transform\n    ref_crs = ref_src.crs\n    ref_width = ref_src.width\n    ref_height = ref_src.height\n    ref_dtype = ref_src.dtypes[0]\n\n# Function to resample rasters to match the reference raster dimensions\ndef resample_raster(src_path):\n    with rasterio.open(src_path) as src:\n        data = src.read(1).astype(float)\n        data[data == src.nodata] = np.nan  # Handle NoData values\n        \n        # Create an empty array for resampled data\n        resampled_data = np.empty((ref_height, ref_width), dtype=ref_dtype)\n        \n        # Reproject and resample\n        reproject(\n            source=data,\n            destination=resampled_data,\n            src_transform=src.transform,\n            src_crs=src.crs,\n            dst_transform=ref_transform,\n            dst_crs=ref_crs,\n            resampling=Resampling.bilinear\n        )\n        \n        return resampled_data\n\n# Read, resample, and apply weights\nweighted_sum = np.zeros((ref_height, ref_width), dtype=np.float32)\n\nfor layer, path in raster_files.items():\n    resampled_raster = resample_raster(path)\n    weighted_sum += weights[layer] * resampled_raster  # Apply weighted sum\n\n# Normalize weighted sum\nweighted_sum = (weighted_sum - np.nanmin(weighted_sum)) / (np.nanmax(weighted_sum) - np.nanmin(weighted_sum))\n\n# Load parcel data\nparcels = gpd.read_file(r\"c:/Users/lorsigno.stu/Documents/Projects/OECM_WebApp/Layers/HoweSound_Parcel_all.shp\")\n\n# Select top X% parcels based on suitability\nthreshold = np.nanpercentile(weighted_sum, 90)\nselected_parcels = parcels.copy()\nselected_parcels[\"suitability\"] = selected_parcels.geometry.centroid.apply(\n    lambda pt: weighted_sum[int(pt.y), int(pt.x)]\n    if 0 &lt;= int(pt.y) &lt; ref_height and 0 &lt;= int(pt.x) &lt; ref_width and weighted_sum[int(pt.y), int(pt.x)] &gt;= threshold\n    else np.nan\n)\nselected_parcels = selected_parcels.dropna()\n\n# Plot results\nfig, ax = plt.subplots(figsize=(10, 8))\n\n# Plot weighted suitability raster\nshow(weighted_sum, transform=ref_transform, cmap=\"viridis\", ax=ax, title=\"Howe Sound Parcel Map\")\n\n# Plot parcels\nparcels.boundary.plot(ax=ax, edgecolor=\"gray\", linewidth=0.5)\n\n# Plot selected parcels\nselected_parcels.boundary.plot(ax=ax, edgecolor=\"red\", linewidth=1.5, label=\"Selected Parcels\")\n\nplt.legend()\nplt.show()\n\n\n\n\n\nHowe Sound Biosphere Parcels"
  },
  {
    "objectID": "cv2.html",
    "href": "cv2.html",
    "title": "CV",
    "section": "",
    "text": "ArcGIS Pro & QGIS\nArcGIS Model Builder and Experience Builder\nFME (Feature Manipulation Engine) for Workflow Automation\nPostgreSQL for Spatial Database Management\nENVI for Remote Sensing Image Analysis\nCartography and Map Design\n\n\n\n\n\nArcGIS Pro, QGIS, ENVI, FME\nPostgreSQL\nPython, R, SQL\nAdobe Illustrator, Photoshop, SketchUp, Procreate\nMicrosoft Office (Excel, Word, PowerPoint)\n\n\n\n\n\nClass 5 Drivers License\nWorkSafeBC\nWHMIS Certified\nFirst Aid (WFA 40 Hour)\nBoating License\nDrone License"
  },
  {
    "objectID": "cv2.html#lead-field-technician-at-aquacoustic-remote-technologies-inc",
    "href": "cv2.html#lead-field-technician-at-aquacoustic-remote-technologies-inc",
    "title": "CV",
    "section": "Lead Field Technician at AquaCoustic Remote Technologies Inc",
    "text": "Lead Field Technician at AquaCoustic Remote Technologies Inc\n(2020-2024)\n\nSetting up, testing and deploying robotic platforms including crawlers and pontoons of different sizes, ROVs, and submersibles to collect video, sonar, laser and additional sensor data\nInspections, cleaning and maintenance of robotic platforms, fiber optic winches, vehicles and other warehouse equipment\nHands-on troubleshooting, assembly and repair of equipment inhouse and in the field\nKeeping accurate daily logs and other detailed inspections sheets and paperwork\nUtility vehicle interior designing, building and maintenance, including office space, electronics and winches\nNightwork and all-weather type field work, working independently, within a team capacity and with other contractors\nProcessing sonar, laser and video data using custom and sourced software"
  },
  {
    "objectID": "cv2.html#co-founder-canopy-campers-van-rentals",
    "href": "cv2.html#co-founder-canopy-campers-van-rentals",
    "title": "CV",
    "section": "Co-Founder Canopy Campers Van Rentals",
    "text": "Co-Founder Canopy Campers Van Rentals\n(2023 - Present)\n\nModelling van interiors using SketchUp for fully off-grid 4-person camper vans\nCustom woodworking and metal working for modular van camping set-ups\nClient interaction & conlfict resolution online and in person, cash and POI transactions\nVehicle maintenance and repairs, regular cleaning, organizing and restocking"
  },
  {
    "objectID": "cv2.html#visitors-service-attendant-for-banff-national-park",
    "href": "cv2.html#visitors-service-attendant-for-banff-national-park",
    "title": "CV",
    "section": "Visitors Service Attendant for Banff National Park",
    "text": "Visitors Service Attendant for Banff National Park\n(Summers 2018 and 2019)\n\nPublic outreach: Providing orientation, information and suggestions to visitors in Banff National Park\nEnforcing Park closures, restrictions and regulations with compliance and ticketing\nEmployee uniform and accessories inventory, distribution and organizing\nUpdating Dispatch and Resource Conservation on wildlife encounters, fires and other public safety concerns\nCustomer service - Handling cash and credit POS transactions in any weather situation quickly and effectively"
  },
  {
    "objectID": "BurnSeverity.html",
    "href": "BurnSeverity.html",
    "title": "Burn Severity",
    "section": "",
    "text": "In this lab, I analyze the burn severity of the 2017 Prouton Lakes wildfire using Landsat 8 time-series data (2013–2021) to assess vegetation loss and post-fire recovery. By computing NDVI and NBR indices, I quantify fire impact and track regrowth over time. Additionally, I classify burn severity using dNBR thresholds and examine recovery patterns across severity classes.\nThis analysis is valuable for forest management, wildfire risk assessment, and ecological monitoring, helping decision-makers understand fire dynamics and long-term landscape recovery. Similar methodologies can be applied to other wildfire-prone regions, post-disaster assessments (e.g., volcanic eruptions, floods), and habitat restoration projects, providing critical insights for conservation and land-use planning.\n\nLoad in necessary packages\n\n#Attach packages here\nlibrary(terra)\n\nWarning: package 'terra' was built under R version 4.3.3\n\nlibrary(sf)\n\nWarning: package 'sf' was built under R version 4.3.3\n\nlibrary(dplyr)\n\nWarning: package 'dplyr' was built under R version 4.3.3\n\nlibrary(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.3.3\n\nlibrary(readr)\n\nWarning: package 'readr' was built under R version 4.3.3\n\nlibrary(stringr)\n\nWarning: package 'stringr' was built under R version 4.3.3\n\nlibrary(lubridate)\n\nWarning: package 'lubridate' was built under R version 4.3.3\n\nlibrary(tidyr)\n\nWarning: package 'tidyr' was built under R version 4.3.3\n\n\n\n\nStep 1 - Historical Fire Analysis:\nIdentify total fires and area burned in 2017. Extract fire IDs and areas for the three largest fires in 2017. Compute the area burned by the Prouton Lakes fire (C30870). Visual Output: Bar plot of total area burned per year in BC, colored by fire cause.\n\n#| execute: true\n#| eval: true\n#| code-fold: true\n#| code-summary: \"R Code Snippet\"\n\n\n# read in the shape file\nfires &lt;- st_read(\"C:/Users/lorsigno.stu/OneDrive - UBC/Desktop/Documents/GEM 520/Lab 8/lab8_finalDemonstrationOfRSkills-assign/data/PROT_HISTORICAL_FIRE_POLYS_SP/H_FIRE_PLY_polygon.shp\")\n\nReading layer `H_FIRE_PLY_polygon' from data source \n  `C:\\Users\\lorsigno.stu\\OneDrive - UBC\\Desktop\\Documents\\GEM 520\\Lab 8\\lab8_finalDemonstrationOfRSkills-assign\\data\\PROT_HISTORICAL_FIRE_POLYS_SP\\H_FIRE_PLY_polygon.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 22479 features and 16 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 408933.1 ymin: 370244.2 xmax: 1870587 ymax: 1709027\nProjected CRS: NAD83 / BC Albers\n\n#filter out the correct year 2017 into another df\nfires_2017 &lt;- fires %&gt;% filter(FIRE_YEAR == 2017)\n#count number of fires within that year\nnumber_fires_2017 &lt;- nrow(fires_2017)\n#calculate the area, in ha, of the fires in 2017\narea_ha_fires_2017 &lt;- sum(fires_2017$SIZE_HA, na.rm = TRUE) #there are no NA values, but I would assume that theres been some preprocessing of this dataset\n\n#find the area of the three largest fires in 2017\n#sort the SIZE_HA into ascending order, extract 3 top rows\nlargest_fires_2017 &lt;- fires_2017 %&gt;% \n  arrange(desc(SIZE_HA)) %&gt;% \n  slice(1:3)\n\n#prouton lake fire is polygon C30870\nprouton_fire &lt;- fires %&gt;% filter(FIRE_NO == 'C30870')\nprouton_fire_area &lt;- prouton_fire$SIZE_HA\n\n\nfires_no_geom &lt;- fires %&gt;% st_drop_geometry()\n\n#plot total area burned per year in BC, colored by fire cause\nburned_area_summary &lt;- fires_no_geom %&gt;% \n  group_by(FIRE_YEAR, FIRE_CAUSE) %&gt;% \n  summarize(total_burned_area = sum(SIZE_HA)) %&gt;%\n  ungroup()\n\n`summarise()` has grouped output by 'FIRE_YEAR'. You can override using the\n`.groups` argument.\n\nggplot(burned_area_summary, aes(x = FIRE_YEAR, y = total_burned_area, fill = FIRE_CAUSE)) +\n  geom_bar(stat = 'identity', position = 'stack') +\n  labs(\n    title = 'Total Area Burned per Year in BC based on Cause',\n    x = 'Year',\n    y = 'Total Area Burned (ha)',\n    fill = 'Fire Cause'\n  ) +\n  theme_minimal()\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_bar()`).\n\n\n\n\n\n\n\n\n\n\n\nStep 2 - Pre-processing Landsat 8 Data:\nSurface reflectance bands (1–7) are combined into multi-layer rasters, with cloud masking using QA_PIXEL values. Data is cropped to the Prouton Lakes fire perimeter. NDVI (Normalized Difference Vegetation Index) and NBR (Normalized Burn Ratio) are calculated. Visual Output: True color composite images before (July 7, 2015) and after the fire (July 15, 2018).\n\n\nR Code Snippet\n# set the base direcory where all the files are stored\nbase_directory &lt;- \"C:/Users/lorsigno.stu/OneDrive - UBC/Desktop/Documents/GEM 520/Lab 8/lab8_finalDemonstrationOfRSkills-assign/data/Landsat 8 OLI_TIRS C2 L2\"\n\n# set the output paths\nSR_dir &lt;- \"C:/Users/lorsigno.stu/OneDrive - UBC/Desktop/Documents/GEM 520/Lab 8/lab8_finalDemonstrationOfRSkills-assign/output/LC08_L2SP_048023_SR\"\nNDVI_dir &lt;- \"C:/Users/lorsigno.stu/OneDrive - UBC/Desktop/Documents/GEM 520/Lab 8/lab8_finalDemonstrationOfRSkills-assign/output/LC08_L2SP_048023_NDVI\"\nNBR_dir &lt;- \"C:/Users/lorsigno.stu/OneDrive - UBC/Desktop/Documents/GEM 520/Lab 8/lab8_finalDemonstrationOfRSkills-assign/output/LC08_L2SP_048023_NBR\"\n\n#listing all files within directory of Landsat tifs\n#flist &lt;- list.dirs(base_directory, full.names = TRUE, recursive = FALSE)\n\nfor (file in list.dirs(base_directory, full.names = TRUE, recursive = FALSE)) {\n  \n  # extract the product ID \n  tif_files &lt;- list.files(file, pattern = \".TIF$\", full.names = TRUE)\n  \n  #if (length(tif_files) &lt; 9) next\n  \n  product_ID &lt;- basename(file)\n  bands &lt;- tif_files[str_detect(basename(tif_files), \"SR_B[1-7]\")]\n  qa_pixel &lt;- tif_files[str_detect(basename(tif_files), \"QA_PIXEL\")]\n  \n  # extract raster bands and qa_pixel\n  sr_stack &lt;- rast(bands)\n  qa_pixel_stack &lt;- rast(qa_pixel)\n  \n  # mask clear conditions\n  sr_mask &lt;- mask(sr_stack, qa_pixel_stack == 21824)\n  \n  # change crs of prouton fire polygon shape from Albers to UTM10N\n  prouton_fire_utm &lt;- st_transform(prouton_fire, crs(sr_stack))\n  \n  # crop to the Proutons Lake Fire extent\n  sr_cropped &lt;- crop(sr_mask, vect(prouton_fire_utm))\n  \n  # calculate NDVI (NIR - red) / (NIR + red)\n  calc_ndvi &lt;- ((sr_cropped[[5]] - sr_cropped[[4]]) / (sr_cropped[[5]] + sr_cropped[[4]]))\n  \n  # calculate NBR (NIR - SWIR) / (NIR + SWIR)\n  calc_nbr &lt;- ((sr_cropped[[5]] - sr_cropped[[7]]) / (sr_cropped[[5]] + sr_cropped[[7]]))\n  \n  sr_output &lt;- paste0(SR_dir, product_ID, \"_SR.tif\")\n  ndvi_output &lt;- paste0(NDVI_dir, product_ID, \"_NDVI.tif\")\n  nbr_output &lt;- paste0(NBR_dir, product_ID, \"_NBR.tif\")\n  \n  writeRaster(sr_cropped, sr_output, overwrite = TRUE)\n  writeRaster(calc_ndvi, ndvi_output, overwrite = TRUE)\n  writeRaster(calc_nbr, nbr_output, overwrite = TRUE)\n\n}\n\n# pre-fire is July 7 2015\n# i need to connect the directory, then assign the bands, then assign the bands a color, crop, then plot\npre_fire_dir &lt;- \"C:/Users/lorsigno.stu/OneDrive - UBC/Desktop/Documents/GEM 520/Lab 8/lab8_finalDemonstrationOfRSkills-assign/output/LC08_L2SP_048023_SRLC08_L2SP_048023_20150707_20200909_02_T1_SR.tif\"\n\npre_fire_RGB &lt;- rast(pre_fire_dir)\n\n# lets crop it to the fire extent\npre_fire_RGB_cropped &lt;- crop(pre_fire_RGB, vect(prouton_fire_utm))\n\n# post-fire is July 15 2018\npost_fire_dir &lt;- \"C:/Users/lorsigno.stu/OneDrive - UBC/Desktop/Documents/GEM 520/Lab 8/lab8_finalDemonstrationOfRSkills-assign/output/LC08_L2SP_048023_SRLC08_L2SP_048023_20180715_20200831_02_T1_SR.tif\"\npost_fire_RGB &lt;- rast(post_fire_dir)\npost_fire_RGB_cropped &lt;- crop(post_fire_RGB, vect(prouton_fire_utm))\n\n# plot\n\nplotRGB(pre_fire_RGB_cropped, r = 4, g = 3, b = 2, stretch=\"lin\")\n\n\n\n\n\n\n\n\n\nR Code Snippet\nplotRGB(post_fire_RGB_cropped, r = 4, g = 3, b = 2, stretch=\"lin\")\n\n\n\n\n\n\n\n\n\n\n\nStep 3 - Yearly NDVI and NBR Composites:\nCompute annual average NDVI and NBR for the burned area. Visual Output: Line plot of mean NDVI over time, showing post-fire recovery trends.\n\n\nR Code Snippet\n# list all the files and make sure they exist\nndvi_list &lt;- list.files(NDVI_dir, pattern = \".tif$\", full.names = TRUE) #flist\nnbr_list &lt;- list.files(NBR_dir, pattern = \".tif$\", full.names = TRUE)\n\nndvi_name &lt;- basename(ndvi_list) # fname\nnbr_name &lt;- basename(nbr_list)\n\n# lets extract the 'text' dates from the ndvi_ and nbr_names\nndvi_date &lt;- str_sub(ndvi_name, start = 39, end = 46)\nnbr_date &lt;- str_sub(nbr_name, start = 38, end = 45)\n\n# we have just the dates stored as a vector, lets convert them to dates\nts_ndvi &lt;- as_date(ndvi_date,\n                   format = \"%Y%m%d\") # ts_dates\nts_nbr &lt;- as_date(nbr_date,\n                   format = \"%Y%m%d\")\n\n# now that we have some dates, lets open the files into a SpatRaster\nndvi_ts &lt;- rast(ndvi_list)\nnbr_ts &lt;- rast(nbr_list)\n\n# initiate an empty list for storing the composites\nndvi_composites_list &lt;- list()\n\nndvi_target_years &lt;- unique(year(ts_ndvi))\n\n# lets get looping!\nfor (year in seq_along(ndvi_target_years)) {\n  \n  # find the target years\n  trg_layers &lt;- which(year(ts_ndvi) == ndvi_target_years[year])\n  \n  # subset the target layers\n  ndvi_trg &lt;- subset(ndvi_ts, trg_layers)\n  \n  # calculate yearly average NDVI\n  ndvi_trg_avg &lt;- app(ndvi_trg, fun = mean, na.rm = TRUE)\n  \n  # assign name based on the year\n  names(ndvi_trg_avg) &lt;- paste0(\"\", ndvi_target_years[year])\n  \n  # add them to the list\n  ndvi_composites_list[[as.character(ndvi_target_years[year])]] &lt;- ndvi_trg_avg\n  \n}\n\nndvi_composite &lt;- rast(ndvi_composites_list)\n\n# lets do the same thing with NBR\nnbr_composite_list &lt;- list()\n\nnbr_target_years &lt;- unique(year(ts_nbr))\n\n# lets get looping!\nfor (y in 1:length(nbr_target_years)) {\n  \n  # find the target years\n  trg_layers &lt;- which(year(ts_nbr) == nbr_target_years[y])\n  \n  # subset the target layers\n  nbr_trg &lt;- subset(nbr_ts, trg_layers)\n  \n  # calculate yearly average NDVI\n  nbr_trg_avg &lt;- app(nbr_trg, fun = mean, na.rm = TRUE)\n  \n  # assign name based on the year\n  names(nbr_trg_avg) &lt;- paste0(\"\", nbr_target_years[y])\n  \n  # add them to the list\n  nbr_composite_list[[y]] &lt;- nbr_trg_avg\n  \n}\nnbr_composite &lt;- rast(nbr_composite_list)\n\nndvi_df &lt;- as.data.frame(ndvi_composite, xy = FALSE, na.rm = TRUE)\n\naverage_ndvi &lt;- colMeans(ndvi_df, na.rm = TRUE)\n\nndvi_summary &lt;- data.frame(\n  Year = as.numeric(gsub(\"Year_\", \"\", names(ndvi_composite))),\n  Average_NDVI = average_ndvi\n)\n\nggplot(ndvi_summary, aes(x = Year, y = Average_NDVI)) +\n  geom_point() +\n  geom_line(group = 1) +\n  labs(\n    title = \"Average NDVI of the Prouton Lakes Fire\",\n    x = \"Year\",\n    y = \"Average NDVI\"\n  ) +\n  theme_minimal() +\n  scale_x_continuous(breaks = seq(min(ndvi_summary$Year),\n                                  max(ndvi_summary$Year),\n                                  by = 1))\n\n\n\n\n\n\n\n\n\n\n\nStep 4 - Burn Severity Classification:\ndNBR (Delta NBR) is calculated using pre-fire (2015) and post-fire (2018) imagery. Burn severity is categorized into unburned, low, medium, and high severity using threshold values. Visual Output: Burn severity map with classification overlay.\n\n\nR Code Snippet\n# set the pre and post fire nbr rasters\npre_fire_nbr &lt;- nbr_composite[[\"2015\"]]\npost_fire_nbr &lt;- nbr_composite[[\"2018\"]]\n\n\n\n# calculate the dNBR\ndNBR &lt;- pre_fire_nbr - post_fire_nbr\n\n# define the classification\nreclass_df &lt;- matrix(c(-0.2, 0.15, 1,\n                       0.15, 0.25, 2,\n                       0.25, 0.3, 3,\n                       0.3, 1.0, 4), ncol = 3, byrow = TRUE)\n\n\n# reclassify dNBR into the severity classes\nburn_severity &lt;- classify(dNBR, reclass_df)\n\n# set the burn severity for the matrix ids 1:4\nlevels(burn_severity) &lt;- data.frame(\n  id = 1:4,\n  label = c(\"unburned\", \"low severity\", \"medium severity\", \"high severity\"))\n\n# plot burn severity with the Prouton Fire Extent with legend\nplot(burn_severity, col = c(\"darkgreen\", \"yellow\", \"orange\", \"red\"),\n     main = \"Burn Severity of the Prouton Lakes Fire\")\nplot(prouton_fire_utm, add = TRUE, border = \"black\", lwd = 2, col = NA)\n\n\nWarning in plot.sf(prouton_fire_utm, add = TRUE, border = \"black\", lwd = 2, :\nignoring all but the first attribute\n\n\n\n\n\n\n\n\n\nR Code Snippet\nwriteRaster(pre_fire_nbr, \"data/pre_fire_nbr.tif\", overwrite = TRUE)\nwriteRaster(post_fire_nbr, \"data/post_fire_nbr.tif\", overwrite = TRUE)\n\n# would have been a good place to look for black morels...\n\n\n\n\nStep 5 - Post-Fire Vegetation Recovery:\nConvert burn severity raster into polygons. Extract NDVI values for each burn severity class from 2018–2021. Visual Outputs: - Yearly NDVI maps from 2018 to 2021 (same value range 0–0.5). - Boxplots showing NDVI distribution by burn severity class over time.\n\n\nR Code Snippet\n# first we need to convert the burn severity rasters to polygons\n# then load in the NDVI rasters from 2018, 2019, 2020 and 2021\n# and then extract the NDVI values by Burn Severity Class\n# then we can plot the yearly NDVI composites\n# finally create some boxplots showing the distribution of NDVI values per burn class by year\n\nburn_severity_masked &lt;- mask(burn_severity, vect(prouton_fire_utm)) # Mask to the fire extent\nburn_severity_polygons &lt;- as.polygons(burn_severity_masked, dissolve = TRUE)\n\nburn_severity_polygons$ID &lt;- 1:nrow(burn_severity_polygons)\n\nyears &lt;- c(2018, 2019, 2020, 2021)\nndvi_values &lt;- list()\n\nfor (year in years) {\n  ndvi_raster &lt;- ndvi_composites_list[[as.character(year)]]\n  \n  extracted_values &lt;- terra::extract(ndvi_raster, burn_severity_polygons,\n                                     xy = TRUE, ID = TRUE)\n  \n  extracted_values &lt;- merge(extracted_values, burn_severity_polygons, by = \"ID\",\n                            all.x = TRUE)\n  \n  colnames(extracted_values)[which(colnames(extracted_values) == as.character(year))] &lt;- \"value\"\n  \n  extracted_values$year &lt;- year\n  ndvi_values[[as.character(year)]] &lt;- extracted_values\n}\n\nndvi_all_years &lt;- do.call(rbind, ndvi_values)\n\nndvi_all_years$severity &lt;- factor(ndvi_all_years$label,\n                                  levels = c('unburned',\n                                             'low severity',\n                                             'medium severity',\n                                             'high severity'),\n                                  ordered = TRUE)\n\n\nfor (year in years) {\n  if (year %in% names(ndvi_composites_list)) {\n    plot(ndvi_composites_list[[as.character(year)]],\n         main = paste(\"Yearly NDVI Composite -\", year),\n         range = c(0, 0.5))\n    }\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR Code Snippet\nggplot(ndvi_all_years, aes(x = severity, y = value, fill = severity)) +\n  geom_boxplot() +\n  facet_wrap(~ year) +\n  scale_fill_manual(values = c(\"unburned\" = \"green\",\n                               \"low severity\" = \"yellow\",\n                               \"medium severity\" = \"orange\",\n                               \"high severity\" = \"red\"),\n                    name = \"Burn Severity\") +\n  labs(title = \"Distribution of NDVI Values by Burn Severity Class\",\n       x = \"Burn Severity Class\",\n       y = \"NDVI\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))"
  },
  {
    "objectID": "OECMParcelSelection.html",
    "href": "OECMParcelSelection.html",
    "title": "OECM Parcel Selection",
    "section": "",
    "text": "This project focuses on identifying and designating Other Effective Area-Based Conservation Measures (OECMs) within the Átl’ḵa7tsem/Howe Sound Biosphere Region. Using geospatial data from BC Parcel Data, CPCAD, and ecological sensitivity datasets, the study aims to assess parcels of land for their potential to support biodiversity conservation while meeting the criteria for OECM designation. The methodology involves creating a “mesh sensitivity layer” that integrates critical habitat data, forest age (&gt;200 years), and proximity to streams, combined with parcel attributes like ownership and zoning.\nSpatial analysis, guided by a decision support tool, evaluates parcels based on ecological sensitivity, proximity to existing conservation areas, and governance capacity. The project also examines differences across regional districts (Metro Vancouver, Sunshine Coast, Squamish Lillooet), aiming to prioritize parcels that are unprotected but ecologically significant. Expected results include a map of high-priority OECM candidates, helping to enhance biodiversity conservation while contributing to Canada’s broader conservation goals under the CBD’s Aichi Targets.\nEach hexagon has a value based on the ecological sensitivity, critical habitat, forest age, proximity to streams and slope steepness. Darker values represent more sensitive areas and are used to select parcels of land from the BC Parcel Fabric layer in order to locate pieces of land that could be considered as OECM.\n\n\nUsing the suitability mesh layer in conjunction with the BC Fabric Parcel layer, I can select parcels that intersect with areas of the mesh where the total_weight is above a specified threshold. In the example below, a conservative value of 2.6, well above the mean, is used to select parcels. The resulting parcels can be seen below. These parcels are then categorized based on their zoning, OCP and municipality. A final list of potential OECMs are given to Howe Sound Biosphere Region Initiative Society (HSBRIS) for them to follow up with those selected parcels and organize an approach to start the land designation change."
  },
  {
    "objectID": "OECMParcelSelection.html#bc-fabric-parcel-selection",
    "href": "OECMParcelSelection.html#bc-fabric-parcel-selection",
    "title": "OECM Parcel Selection",
    "section": "",
    "text": "Using the suitability mesh layer in conjunction with the BC Fabric Parcel layer, I can select parcels that intersect with areas of the mesh where the total_weight is above a specified threshold. In the example below, a conservative value of 2.6, well above the mean, is used to select parcels. The resulting parcels can be seen below. These parcels are then categorized based on their zoning, OCP and municipality. A final list of potential OECMs are given to Howe Sound Biosphere Region Initiative Society (HSBRIS) for them to follow up with those selected parcels and organize an approach to start the land designation change."
  },
  {
    "objectID": "OECMParcelSelection.html#python",
    "href": "OECMParcelSelection.html#python",
    "title": "OECM Parcel Selection",
    "section": "Python",
    "text": "Python\n\n\nPython Code Snippet\nimport rasterio\nimport numpy as np\nimport geopandas as gpd\nimport matplotlib.pyplot as plt\nfrom rasterio.plot import show\nfrom rasterio.warp import reproject, Resampling\n\n# Define input raster files\nraster_files = {\n    \"biological_sensitivity\": r\"C:/Users/lorsigno.stu/Documents/GitHub/E-Portfolio/data/Layers/critical_habitat_raster.tif\",\n    \"watershed\": r\"C:/Users/lorsigno.stu/Documents/GitHub/E-Portfolio/data/Layers/watershed_raster.tif\",\n    \"streams\": r\"C:/Users/lorsigno.stu/Documents/GitHub/E-Portfolio/data/Layers/streams_50m_buffer.tif\",\n    \"wetlands\": r\"C:/Users/lorsigno.stu/Documents/GitHub/E-Portfolio/data/Layers/wetland_raster.tif\",\n    \"slope\": r\"C:/Users/lorsigno.stu/Documents/GitHub/E-Portfolio/data/Layers/slope_greaterthan30_raster.tif\",\n    \"forest_age\": r\"C:/Users/lorsigno.stu/Documents/GitHub/E-Portfolio/data/Layers/forest_age_vri.tif\"\n}\n\n# Define weights\nweights = {\n    \"biological_sensitivity\": 0.25,\n    \"watershed\": 0.15,\n    \"streams\": 0.2,\n    \"wetlands\": 0.15,\n    \"slope\": 0.1,\n    \"forest_age\": 0.15\n}\n\n# Select a reference raster (first raster in the dictionary)\nreference_raster = list(raster_files.values())[0]\n\n# Read reference raster to get target shape, transform, and CRS\nwith rasterio.open(reference_raster) as ref_src:\n    ref_transform = ref_src.transform\n    ref_crs = ref_src.crs\n    ref_width = ref_src.width\n    ref_height = ref_src.height\n    ref_dtype = ref_src.dtypes[0]\n\n# Function to resample rasters to match the reference raster dimensions\ndef resample_raster(src_path):\n    with rasterio.open(src_path) as src:\n        data = src.read(1).astype(float)\n        data[data == src.nodata] = np.nan  # Handle NoData values\n        \n        # Create an empty array for resampled data\n        resampled_data = np.empty((ref_height, ref_width), dtype=ref_dtype)\n        \n        # Reproject and resample\n        reproject(\n            source=data,\n            destination=resampled_data,\n            src_transform=src.transform,\n            src_crs=src.crs,\n            dst_transform=ref_transform,\n            dst_crs=ref_crs,\n            resampling=Resampling.bilinear\n        )\n        \n        return resampled_data\n\n# Read, resample, and apply weights\nweighted_sum = np.zeros((ref_height, ref_width), dtype=np.float32)\n\nfor layer, path in raster_files.items():\n    resampled_raster = resample_raster(path)\n    weighted_sum += weights[layer] * resampled_raster  # Apply weighted sum\n\n# Normalize weighted sum\nweighted_sum = (weighted_sum - np.nanmin(weighted_sum)) / (np.nanmax(weighted_sum) - np.nanmin(weighted_sum))\n\n# Load parcel data\nparcels = gpd.read_file(r\"c:/Users/lorsigno.stu/Documents/Projects/OECM_WebApp/Layers/HoweSound_Parcel_all.shp\")\n\n# Select top X% parcels based on suitability\nthreshold = np.nanpercentile(weighted_sum, 90)\nselected_parcels = parcels.copy()\nselected_parcels[\"suitability\"] = selected_parcels.geometry.centroid.apply(\n    lambda pt: weighted_sum[int(pt.y), int(pt.x)]\n    if 0 &lt;= int(pt.y) &lt; ref_height and 0 &lt;= int(pt.x) &lt; ref_width and weighted_sum[int(pt.y), int(pt.x)] &gt;= threshold\n    else np.nan\n)\nselected_parcels = selected_parcels.dropna()\n\n# Plot results\nfig, ax = plt.subplots(figsize=(10, 8))\n\n# Plot weighted suitability raster\nshow(weighted_sum, transform=ref_transform, cmap=\"viridis\", ax=ax, title=\"Howe Sound Parcel Map\")\n\n# Plot parcels\nparcels.boundary.plot(ax=ax, edgecolor=\"gray\", linewidth=0.5)\n\n# Plot selected parcels\nselected_parcels.boundary.plot(ax=ax, edgecolor=\"red\", linewidth=1.5, label=\"Selected Parcels\")\n\nplt.legend()\nplt.show()\n\n\n\n\n\nHowe Sound Biosphere Parcels"
  },
  {
    "objectID": "classification.html",
    "href": "classification.html",
    "title": "Classification",
    "section": "",
    "text": "During my free time and from my Masters of Geomatics for Environmental Management (MGEM) at UBC, I have learned a range of remote sensing techniques and skills.\nBelow are some of those projects, current and past."
  },
  {
    "objectID": "classification.html#land-classification",
    "href": "classification.html#land-classification",
    "title": "Classification",
    "section": "Land Classification",
    "text": "Land Classification\nThe Remote Sensing course in MGEM taught me how to produce landscape classifications using a variety of supervised and unsupervised classification processes. Fascinated by different classification processes and the maps it produced, I extended this skill to produce a number of classified maps of various landscapes where Landsat and Copernicus satellite data was available. Generally my focus was with the Howe Sound Biosphere as it was part of my capstone project at UBC and is where I spend my time hiking, skiing and climbing. Below are some of the results of recent pixel classifications.\nBelow is a slide-by-side comparison of two different classification processes of the Howe Sound. The left side is classified under a supervised process using hand-delineated polygons, 20/80 training to validation, with custom R code (a lot of work). The right shows a classification using the ArcGIS Pro deeplearning land cover classification (almost instantaneous).\n\n\n\n\n\n\nThe idea here is to visualize the difference and address the accuracies between the two classification models. How do you think they compare?"
  },
  {
    "objectID": "BurnSeverity_Modified.html",
    "href": "BurnSeverity_Modified.html",
    "title": "Burn Severity",
    "section": "",
    "text": "In this lab, I analyze the burn severity of the 2017 Prouton Lakes wildfire using Landsat 8 time-series data (2013–2021) to assess vegetation loss and post-fire recovery. By computing NDVI and NBR indices, I quantify fire impact and track regrowth over time. Additionally, I classify burn severity using dNBR thresholds and examine recovery patterns across severity classes.\nThis analysis is valuable for forest management, wildfire risk assessment, and ecological monitoring, helping decision-makers understand fire dynamics and long-term landscape recovery. Similar methodologies can be applied to other wildfire-prone regions, post-disaster assessments (e.g., volcanic eruptions, floods), and habitat restoration projects, providing critical insights for conservation and land-use planning.\n\nLoad in necessary packages\n\n\nCode\n#| code-fold: true\n#| code-summary: \"Show/Hide Code\"\n\n#Attach packages\nsuppressPackageStartupMessages({\n  library(terra)\n  library(sf)\n  library(dplyr)\n  library(ggplot2)\n  library(readr)\n  library(stringr)\n  library(lubridate)\n  library(tidyr)\n})\n\n\n\n\nStep 1 - Historical Fire Analysis:\nIdentify total fires and area burned in 2017. Extract fire IDs and areas for the three largest fires in 2017. Compute the area burned by the Prouton Lakes fire (C30870). Visual Output: Bar plot of total area burned per year in BC, colored by fire cause.\n\n\nCode\n#| execute: true\n#| eval: true\n#| echo: false\n#| code-fold: true\n#| code-summary: \"Show/Hide Code\"\n\n\n#read in the shape file\nfires &lt;- st_read(\"C:/Users/lorsigno.stu/OneDrive - UBC/Desktop/Documents/GEM 520/Lab 8/lab8_finalDemonstrationOfRSkills-assign/data/PROT_HISTORICAL_FIRE_POLYS_SP/H_FIRE_PLY_polygon.shp\")\n\n\nReading layer `H_FIRE_PLY_polygon' from data source \n  `C:\\Users\\lorsigno.stu\\OneDrive - UBC\\Desktop\\Documents\\GEM 520\\Lab 8\\lab8_finalDemonstrationOfRSkills-assign\\data\\PROT_HISTORICAL_FIRE_POLYS_SP\\H_FIRE_PLY_polygon.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 22479 features and 16 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 408933.1 ymin: 370244.2 xmax: 1870587 ymax: 1709027\nProjected CRS: NAD83 / BC Albers\n\n\nCode\n#filter out the correct year 2017 into another df\nfires_2017 &lt;- fires %&gt;% filter(FIRE_YEAR == 2017)\n#count number of fires within that year\nnumber_fires_2017 &lt;- nrow(fires_2017)\n#calculate the area, in ha, of the fires in 2017\narea_ha_fires_2017 &lt;- sum(fires_2017$SIZE_HA, na.rm = TRUE) #there are no NA values, but I would assume that theres been some preprocessing of this dataset\n\n#find the area of the three largest fires in 2017\n#sort the SIZE_HA into ascending order, extract 3 top rows\nlargest_fires_2017 &lt;- fires_2017 %&gt;% \n  arrange(desc(SIZE_HA)) %&gt;% \n  slice(1:3)\n\n#prouton lake fire is polygon C30870\nprouton_fire &lt;- fires %&gt;% filter(FIRE_NO == 'C30870')\nprouton_fire_area &lt;- prouton_fire$SIZE_HA\n\n\nfires_no_geom &lt;- fires %&gt;% st_drop_geometry()\n\n#plot total area burned per year in BC, colored by fire cause\nburned_area_summary &lt;- fires_no_geom %&gt;% \n  group_by(FIRE_YEAR, FIRE_CAUSE) %&gt;% \n  summarize(total_burned_area = sum(SIZE_HA)) %&gt;%\n  ungroup()\n\nggplot(burned_area_summary, aes(x = FIRE_YEAR, y = total_burned_area, fill = FIRE_CAUSE)) +\n  geom_bar(stat = 'identity', position = 'stack') +\n  labs(\n    title = 'Total Area Burned per Year in BC based on Cause',\n    x = 'Year',\n    y = 'Total Area Burned (ha)',\n    fill = 'Fire Cause'\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nStep 2 - Pre-processing Landsat 8 Data:\nSurface reflectance bands (1–7) are combined into multi-layer rasters, with cloud masking using QA_PIXEL values. Data is cropped to the Prouton Lakes fire perimeter. NDVI (Normalized Difference Vegetation Index) and NBR (Normalized Burn Ratio) are calculated. Visual Output: True color composite images before (July 7, 2015) and after the fire (July 15, 2018).\n\n\nCode\n#| execute: true\n#| eval: true\n#| warning: false\n#| message: false\n#| code-fold: true\n#| code-summary: \"Show/Hide Code\"\n\n# set the base direcory where all the files are stored\nbase_directory &lt;- \"C:/Users/lorsigno.stu/OneDrive - UBC/Desktop/Documents/GEM 520/Lab 8/lab8_finalDemonstrationOfRSkills-assign/data/Landsat 8 OLI_TIRS C2 L2\"\n\n# set the output paths\nSR_dir &lt;- \"C:/Users/lorsigno.stu/OneDrive - UBC/Desktop/Documents/GEM 520/Lab 8/lab8_finalDemonstrationOfRSkills-assign/output/LC08_L2SP_048023_SR\"\nNDVI_dir &lt;- \"C:/Users/lorsigno.stu/OneDrive - UBC/Desktop/Documents/GEM 520/Lab 8/lab8_finalDemonstrationOfRSkills-assign/output/LC08_L2SP_048023_NDVI\"\nNBR_dir &lt;- \"C:/Users/lorsigno.stu/OneDrive - UBC/Desktop/Documents/GEM 520/Lab 8/lab8_finalDemonstrationOfRSkills-assign/output/LC08_L2SP_048023_NBR\"\n\n#listing all files within directory of Landsat tifs\n#flist &lt;- list.dirs(base_directory, full.names = TRUE, recursive = FALSE)\n\nfor (file in list.dirs(base_directory, full.names = TRUE, recursive = FALSE)) {\n  \n  # extract the product ID \n  tif_files &lt;- list.files(file, pattern = \".TIF$\", full.names = TRUE)\n  \n  #if (length(tif_files) &lt; 9) next\n  \n  product_ID &lt;- basename(file)\n  bands &lt;- tif_files[str_detect(basename(tif_files), \"SR_B[1-7]\")]\n  qa_pixel &lt;- tif_files[str_detect(basename(tif_files), \"QA_PIXEL\")]\n  \n  # extract raster bands and qa_pixel\n  sr_stack &lt;- rast(bands)\n  qa_pixel_stack &lt;- rast(qa_pixel)\n  \n  # mask clear conditions\n  sr_mask &lt;- mask(sr_stack, qa_pixel_stack == 21824)\n  \n  # change crs of prouton fire polygon shape from Albers to UTM10N\n  prouton_fire_utm &lt;- st_transform(prouton_fire, crs(sr_stack))\n  \n  # crop to the Proutons Lake Fire extent\n  sr_cropped &lt;- crop(sr_mask, vect(prouton_fire_utm))\n  \n  # calculate NDVI (NIR - red) / (NIR + red)\n  calc_ndvi &lt;- ((sr_cropped[[5]] - sr_cropped[[4]]) / (sr_cropped[[5]] + sr_cropped[[4]]))\n  \n  # calculate NBR (NIR - SWIR) / (NIR + SWIR)\n  calc_nbr &lt;- ((sr_cropped[[5]] - sr_cropped[[7]]) / (sr_cropped[[5]] + sr_cropped[[7]]))\n  \n  sr_output &lt;- paste0(SR_dir, product_ID, \"_SR.tif\")\n  ndvi_output &lt;- paste0(NDVI_dir, product_ID, \"_NDVI.tif\")\n  nbr_output &lt;- paste0(NBR_dir, product_ID, \"_NBR.tif\")\n  \n  writeRaster(sr_cropped, sr_output, overwrite = TRUE)\n  writeRaster(calc_ndvi, ndvi_output, overwrite = TRUE)\n  writeRaster(calc_nbr, nbr_output, overwrite = TRUE)\n\n}\n\n# pre-fire is July 7 2015\n# i need to connect the directory, then assign the bands, then assign the bands a color, crop, then plot\npre_fire_dir &lt;- \"C:/Users/lorsigno.stu/OneDrive - UBC/Desktop/Documents/GEM 520/Lab 8/lab8_finalDemonstrationOfRSkills-assign/output/LC08_L2SP_048023_SRLC08_L2SP_048023_20150707_20200909_02_T1_SR.tif\"\n\npre_fire_RGB &lt;- rast(pre_fire_dir)\n\n# lets crop it to the fire extent\npre_fire_RGB_cropped &lt;- crop(pre_fire_RGB, vect(prouton_fire_utm))\n\n# post-fire is July 15 2018\npost_fire_dir &lt;- \"C:/Users/lorsigno.stu/OneDrive - UBC/Desktop/Documents/GEM 520/Lab 8/lab8_finalDemonstrationOfRSkills-assign/output/LC08_L2SP_048023_SRLC08_L2SP_048023_20180715_20200831_02_T1_SR.tif\"\npost_fire_RGB &lt;- rast(post_fire_dir)\npost_fire_RGB_cropped &lt;- crop(post_fire_RGB, vect(prouton_fire_utm))\n\n# plot\n\nplotRGB(pre_fire_RGB_cropped, r = 4, g = 3, b = 2, stretch=\"lin\")\n\n\n\n\n\n\n\n\n\nCode\nplotRGB(post_fire_RGB_cropped, r = 4, g = 3, b = 2, stretch=\"lin\")\n\n\n\n\n\n\n\n\n\n\n\nStep 3 - Yearly NDVI and NBR Composites:\nCompute annual average NDVI and NBR for the burned area. Visual Output: Line plot of mean NDVI over time, showing post-fire recovery trends.\n\n\nCode\n#| execute: true\n#| eval: true\n#| warning: false\n#| message: false\n#| code-fold: true\n#| code-summary: \"Show/Hide Code\"\n\n# list all the files and make sure they exist\nndvi_list &lt;- list.files(NDVI_dir, pattern = \".tif$\", full.names = TRUE) #flist\nnbr_list &lt;- list.files(NBR_dir, pattern = \".tif$\", full.names = TRUE)\n\nndvi_name &lt;- basename(ndvi_list) # fname\nnbr_name &lt;- basename(nbr_list)\n\n# lets extract the 'text' dates from the ndvi_ and nbr_names\nndvi_date &lt;- str_sub(ndvi_name, start = 39, end = 46)\nnbr_date &lt;- str_sub(nbr_name, start = 38, end = 45)\n\n# we have just the dates stored as a vector, lets convert them to dates\nts_ndvi &lt;- as_date(ndvi_date,\n                   format = \"%Y%m%d\") # ts_dates\nts_nbr &lt;- as_date(nbr_date,\n                   format = \"%Y%m%d\")\n\n# now that we have some dates, lets open the files into a SpatRaster\nndvi_ts &lt;- rast(ndvi_list)\nnbr_ts &lt;- rast(nbr_list)\n\n# initiate an empty list for storing the composites\nndvi_composites_list &lt;- list()\n\nndvi_target_years &lt;- unique(year(ts_ndvi))\n\n# lets get looping!\nfor (year in seq_along(ndvi_target_years)) {\n  \n  # find the target years\n  trg_layers &lt;- which(year(ts_ndvi) == ndvi_target_years[year])\n  \n  # subset the target layers\n  ndvi_trg &lt;- subset(ndvi_ts, trg_layers)\n  \n  # calculate yearly average NDVI\n  ndvi_trg_avg &lt;- app(ndvi_trg, fun = mean, na.rm = TRUE)\n  \n  # assign name based on the year\n  names(ndvi_trg_avg) &lt;- paste0(\"\", ndvi_target_years[year])\n  \n  # add them to the list\n  ndvi_composites_list[[as.character(ndvi_target_years[year])]] &lt;- ndvi_trg_avg\n  \n}\n\nndvi_composite &lt;- rast(ndvi_composites_list)\n\n# lets do the same thing with NBR\nnbr_composite_list &lt;- list()\n\nnbr_target_years &lt;- unique(year(ts_nbr))\n\n# lets get looping!\nfor (y in 1:length(nbr_target_years)) {\n  \n  # find the target years\n  trg_layers &lt;- which(year(ts_nbr) == nbr_target_years[y])\n  \n  # subset the target layers\n  nbr_trg &lt;- subset(nbr_ts, trg_layers)\n  \n  # calculate yearly average NDVI\n  nbr_trg_avg &lt;- app(nbr_trg, fun = mean, na.rm = TRUE)\n  \n  # assign name based on the year\n  names(nbr_trg_avg) &lt;- paste0(\"\", nbr_target_years[y])\n  \n  # add them to the list\n  nbr_composite_list[[y]] &lt;- nbr_trg_avg\n  \n}\nnbr_composite &lt;- rast(nbr_composite_list)\n\nndvi_df &lt;- as.data.frame(ndvi_composite, xy = FALSE, na.rm = TRUE)\n\naverage_ndvi &lt;- colMeans(ndvi_df, na.rm = TRUE)\n\nndvi_summary &lt;- data.frame(\n  Year = as.numeric(gsub(\"Year_\", \"\", names(ndvi_composite))),\n  Average_NDVI = average_ndvi\n)\n\nggplot(ndvi_summary, aes(x = Year, y = Average_NDVI)) +\n  geom_point() +\n  geom_line(group = 1) +\n  labs(\n    title = \"Average NDVI of the Prouton Lakes Fire\",\n    x = \"Year\",\n    y = \"Average NDVI\"\n  ) +\n  theme_minimal() +\n  scale_x_continuous(breaks = seq(min(ndvi_summary$Year),\n                                  max(ndvi_summary$Year),\n                                  by = 1))\n\n\n\n\n\n\n\n\n\n\n\nStep 4 - Burn Severity Classification:\ndNBR (Delta NBR) is calculated using pre-fire (2015) and post-fire (2018) imagery. Burn severity is categorized into unburned, low, medium, and high severity using threshold values. Visual Output: Burn severity map with classification overlay.\n\n\nCode\n#| execute: true\n#| eval: true\n#| warning: false\n#| message: false\n#| code-fold: true\n#| code-summary: \"Show/Hide Code\"\n\n# set the pre and post fire nbr rasters\npre_fire_nbr &lt;- nbr_composite[[\"2015\"]]\npost_fire_nbr &lt;- nbr_composite[[\"2018\"]]\n\n\n\n# calculate the dNBR\ndNBR &lt;- pre_fire_nbr - post_fire_nbr\n\n# define the classification\nreclass_df &lt;- matrix(c(-0.2, 0.15, 1,\n                       0.15, 0.25, 2,\n                       0.25, 0.3, 3,\n                       0.3, 1.0, 4), ncol = 3, byrow = TRUE)\n\n\n# reclassify dNBR into the severity classes\nburn_severity &lt;- classify(dNBR, reclass_df)\n\n# set the burn severity for the matrix ids 1:4\nlevels(burn_severity) &lt;- data.frame(\n  id = 1:4,\n  label = c(\"unburned\", \"low severity\", \"medium severity\", \"high severity\"))\n\n# plot burn severity with the Prouton Fire Extent with legend\nplot(burn_severity, col = c(\"darkgreen\", \"yellow\", \"orange\", \"red\"),\n     main = \"Burn Severity of the Prouton Lakes Fire\")\nplot(prouton_fire_utm, add = TRUE, border = \"black\", lwd = 2, col = NA)\n\n\nWarning in plot.sf(prouton_fire_utm, add = TRUE, border = \"black\", lwd = 2, :\nignoring all but the first attribute\n\n\n\n\n\n\n\n\n\nCode\n# would have been a good place to look for black morels...\n\n\n\n\nStep 5 - Post-Fire Vegetation Recovery:\nConvert burn severity raster into polygons. Extract NDVI values for each burn severity class from 2018–2021. Visual Outputs: - Yearly NDVI maps from 2018 to 2021 (same value range 0–0.5). - Boxplots showing NDVI distribution by burn severity class over time.\n\n\nCode\n#| execute: true\n#| eval: true\n#| warning: false\n#| message: false\n#| code-fold: true\n#| code-summary: \"Show/Hide Code\"\n\n# first we need to convert the burn severity rasters to polygons\n# then load in the NDVI rasters from 2018, 2019, 2020 and 2021\n# and then extract the NDVI values by Burn Severity Class\n# then we can plot the yearly NDVI composites\n# finally create some boxplots showing the distribution of NDVI values per burn class by year\n\nburn_severity_masked &lt;- mask(burn_severity, vect(prouton_fire_utm)) # Mask to the fire extent\nburn_severity_polygons &lt;- as.polygons(burn_severity_masked, dissolve = TRUE)\n\nburn_severity_polygons$ID &lt;- 1:nrow(burn_severity_polygons)\n\nyears &lt;- c(2018, 2019, 2020, 2021)\nndvi_values &lt;- list()\n\nfor (year in years) {\n  ndvi_raster &lt;- ndvi_composites_list[[as.character(year)]]\n  \n  extracted_values &lt;- terra::extract(ndvi_raster, burn_severity_polygons,\n                                     xy = TRUE, ID = TRUE)\n  \n  extracted_values &lt;- merge(extracted_values, burn_severity_polygons, by = \"ID\",\n                            all.x = TRUE)\n  \n  colnames(extracted_values)[which(colnames(extracted_values) == as.character(year))] &lt;- \"value\"\n  \n  extracted_values$year &lt;- year\n  ndvi_values[[as.character(year)]] &lt;- extracted_values\n}\n\nndvi_all_years &lt;- do.call(rbind, ndvi_values)\n\nndvi_all_years$severity &lt;- factor(ndvi_all_years$label,\n                                  levels = c('unburned',\n                                             'low severity',\n                                             'medium severity',\n                                             'high severity'),\n                                  ordered = TRUE)\n\n\nfor (year in years) {\n  if (year %in% names(ndvi_composites_list)) {\n    plot(ndvi_composites_list[[as.character(year)]],\n         main = paste(\"Yearly NDVI Composite -\", year),\n         range = c(0, 0.5))\n    }\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(ndvi_all_years, aes(x = severity, y = value, fill = severity)) +\n  geom_boxplot() +\n  facet_wrap(~ year) +\n  scale_fill_manual(values = c(\"unburned\" = \"green\",\n                               \"low severity\" = \"yellow\",\n                               \"medium severity\" = \"orange\",\n                               \"high severity\" = \"red\"),\n                    name = \"Burn Severity\") +\n  labs(title = \"Distribution of NDVI Values by Burn Severity Class\",\n       x = \"Burn Severity Class\",\n       y = \"NDVI\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))"
  },
  {
    "objectID": "burnseverity.html",
    "href": "burnseverity.html",
    "title": "Burn Severity Analysis of Prouton Lakes Fire",
    "section": "",
    "text": "In this lab, I analyze the burn severity of the 2017 Prouton Lakes wildfire using Landsat 8 time-series data (2013–2021) to assess vegetation loss and post-fire recovery. By computing NDVI and NBR indices, I quantify fire impact and track regrowth over time. Additionally, I classify burn severity using dNBR thresholds and examine recovery patterns across severity classes.\nThis analysis is valuable for forest management, wildfire risk assessment, and ecological monitoring, helping decision-makers understand fire dynamics and long-term landscape recovery. Similar methodologies can be applied to other wildfire-prone regions, post-disaster assessments (e.g., volcanic eruptions, floods), and habitat restoration projects, providing critical insights for conservation and land-use planning.\n\nLoad in necessary packages\n\n\nCode\n#| code-fold: true\n#| code-summary: \"Show/Hide Code\"\n\n#Attach packages\nsuppressPackageStartupMessages({\n  library(terra)\n  library(sf)\n  library(dplyr)\n  library(ggplot2)\n  library(readr)\n  library(stringr)\n  library(lubridate)\n  library(tidyr)\n})\n\n\n\n\nStep 1 - Historical Fire Analysis:\nIdentify total fires and area burned in 2017. Extract fire IDs and areas for the three largest fires in 2017. Compute the area burned by the Prouton Lakes fire (C30870). Visual Output: Bar plot of total area burned per year in BC, colored by fire cause.\n\n\nCode\n#| execute: true\n#| eval: true\n#| echo: false\n#| warning: false\n#| message: false\n#| code-fold: true\n#| code-summary: \"Show/Hide Code\"\n\n#read in the shape file\nfires &lt;- st_read(\"C:/Users/lorsigno.stu/OneDrive - UBC/Desktop/Documents/GEM 520/Lab 8/lab8_finalDemonstrationOfRSkills-assign/data/PROT_HISTORICAL_FIRE_POLYS_SP/H_FIRE_PLY_polygon.shp\", quiet = TRUE)\n\n#filter out the correct year 2017 into another df\nfires_2017 &lt;- fires %&gt;% filter(FIRE_YEAR == 2017)\n#count number of fires within that year\nnumber_fires_2017 &lt;- nrow(fires_2017)\n#calculate the area, in ha, of the fires in 2017\narea_ha_fires_2017 &lt;- sum(fires_2017$SIZE_HA, na.rm = TRUE) #there are no NA values, but I would assume that theres been some preprocessing of this dataset\n\n#find the area of the three largest fires in 2017\n#sort the SIZE_HA into ascending order, extract 3 top rows\nlargest_fires_2017 &lt;- fires_2017 %&gt;% \n  arrange(desc(SIZE_HA)) %&gt;% \n  slice(1:3)\n\n#prouton lake fire is polygon C30870\nprouton_fire &lt;- fires %&gt;% filter(FIRE_NO == 'C30870')\nprouton_fire_area &lt;- prouton_fire$SIZE_HA\n\n\nfires_no_geom &lt;- fires %&gt;% st_drop_geometry()\n\n#plot total area burned per year in BC, colored by fire cause\nburned_area_summary &lt;- fires_no_geom %&gt;% \n  group_by(FIRE_YEAR, FIRE_CAUSE) %&gt;% \n  summarize(total_burned_area = sum(SIZE_HA)) %&gt;%\n  ungroup()\n\nggplot(burned_area_summary, aes(x = FIRE_YEAR, y = total_burned_area, fill = FIRE_CAUSE)) +\n  geom_bar(stat = 'identity', position = 'stack') +\n  labs(\n    title = 'Total Area Burned per Year in BC based on Cause',\n    x = 'Year',\n    y = 'Total Area Burned (ha)',\n    fill = 'Fire Cause'\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nStep 2 - Pre-processing Landsat 8 Data:\nSurface reflectance bands (1–7) are combined into multi-layer rasters, with cloud masking using QA_PIXEL values. Data is cropped to the Prouton Lakes fire perimeter. NDVI (Normalized Difference Vegetation Index) and NBR (Normalized Burn Ratio) are calculated. Visual Output: True color composite images before (July 7, 2015) and after the fire (July 15, 2018).\n\n\nCode\n#| execute: true\n#| eval: true\n#| warning: false\n#| message: false\n#| code-fold: true\n#| code-summary: \"Show/Hide Code\"\n\n# set the base direcory where all the files are stored\nbase_directory &lt;- \"C:/Users/lorsigno.stu/OneDrive - UBC/Desktop/Documents/GEM 520/Lab 8/lab8_finalDemonstrationOfRSkills-assign/data/Landsat 8 OLI_TIRS C2 L2\"\n\n# set the output paths\nSR_dir &lt;- \"C:/Users/lorsigno.stu/OneDrive - UBC/Desktop/Documents/GEM 520/Lab 8/lab8_finalDemonstrationOfRSkills-assign/output/LC08_L2SP_048023_SR\"\nNDVI_dir &lt;- \"C:/Users/lorsigno.stu/OneDrive - UBC/Desktop/Documents/GEM 520/Lab 8/lab8_finalDemonstrationOfRSkills-assign/output/LC08_L2SP_048023_NDVI\"\nNBR_dir &lt;- \"C:/Users/lorsigno.stu/OneDrive - UBC/Desktop/Documents/GEM 520/Lab 8/lab8_finalDemonstrationOfRSkills-assign/output/LC08_L2SP_048023_NBR\"\n\n#listing all files within directory of Landsat tifs\n#flist &lt;- list.dirs(base_directory, full.names = TRUE, recursive = FALSE)\n\nfor (file in list.dirs(base_directory, full.names = TRUE, recursive = FALSE)) {\n  \n  # extract the product ID \n  tif_files &lt;- list.files(file, pattern = \".TIF$\", full.names = TRUE)\n  \n  #if (length(tif_files) &lt; 9) next\n  \n  product_ID &lt;- basename(file)\n  bands &lt;- tif_files[str_detect(basename(tif_files), \"SR_B[1-7]\")]\n  qa_pixel &lt;- tif_files[str_detect(basename(tif_files), \"QA_PIXEL\")]\n  \n  # extract raster bands and qa_pixel\n  sr_stack &lt;- rast(bands)\n  qa_pixel_stack &lt;- rast(qa_pixel)\n  \n  # mask clear conditions\n  sr_mask &lt;- mask(sr_stack, qa_pixel_stack == 21824)\n  \n  # change crs of prouton fire polygon shape from Albers to UTM10N\n  prouton_fire_utm &lt;- st_transform(prouton_fire, crs(sr_stack))\n  \n  # crop to the Proutons Lake Fire extent\n  sr_cropped &lt;- crop(sr_mask, vect(prouton_fire_utm))\n  \n  # calculate NDVI (NIR - red) / (NIR + red)\n  calc_ndvi &lt;- ((sr_cropped[[5]] - sr_cropped[[4]]) / (sr_cropped[[5]] + sr_cropped[[4]]))\n  \n  # calculate NBR (NIR - SWIR) / (NIR + SWIR)\n  calc_nbr &lt;- ((sr_cropped[[5]] - sr_cropped[[7]]) / (sr_cropped[[5]] + sr_cropped[[7]]))\n  \n  #sr_output &lt;- paste0(SR_dir, product_ID, \"_SR.tif\")\n  #ndvi_output &lt;- paste0(NDVI_dir, product_ID, \"_NDVI.tif\")\n  #nbr_output &lt;- paste0(NBR_dir, product_ID, \"_NBR.tif\")\n  \n  #writeRaster(sr_cropped, sr_output, overwrite = TRUE)\n  #writeRaster(calc_ndvi, ndvi_output, overwrite = TRUE)\n  #writeRaster(calc_nbr, nbr_output, overwrite = TRUE)\n\n}\n\n# pre-fire is July 7 2015\npre_fire_RGB &lt;- rast(\"~/GEM 520/Lab 8/lab8_finalDemonstrationOfRSkills-assign/output/LC08_L2SP_048023_SRLC08_L2SP_048023_20150707_20200909_02_T1_SR.tif\")\n# lets crop it to the fire extent\npre_fire_RGB_cropped &lt;- crop(pre_fire_RGB, vect(prouton_fire_utm))\nplotRGB(pre_fire_RGB_cropped, r = 4, g = 3, b = 2, stretch = \"lin\", main = \"Pre-Fire (2015)\")\n\n\n\n\n\n\n\n\n\nCode\n# post-fire is July 15 2018\npost_fire_RGB &lt;- rast(\"~/GEM 520/Lab 8/lab8_finalDemonstrationOfRSkills-assign/output/LC08_L2SP_048023_SRLC08_L2SP_048023_20180715_20200831_02_T1_SR.tif\")\npost_fire_RGB_cropped &lt;- crop(post_fire_RGB, vect(prouton_fire_utm))\nplotRGB(post_fire_RGB_cropped, r = 4, g = 3, b = 2, stretch = \"lin\", main = \"Post-Fire (2018)\")\n\n\n\n\n\n\n\n\n\n\n\nStep 3 - Yearly NDVI and NBR Composites:\nCompute annual average NDVI and NBR for the burned area. Visual Output: Line plot of mean NDVI over time, showing post-fire recovery trends.\n\n\nCode\n#| execute: true\n#| eval: true\n#| warning: false\n#| message: false\n#| code-fold: true\n#| code-summary: \"Show/Hide Code\"\n\n# list all the files and make sure they exist\nndvi_list &lt;- list.files(NDVI_dir, pattern = \".tif$\", full.names = TRUE) #flist\nnbr_list &lt;- list.files(NBR_dir, pattern = \".tif$\", full.names = TRUE)\n\nndvi_name &lt;- basename(ndvi_list) # fname\nnbr_name &lt;- basename(nbr_list)\n\n# lets extract the 'text' dates from the ndvi_ and nbr_names\nndvi_date &lt;- str_sub(ndvi_name, start = 39, end = 46)\nnbr_date &lt;- str_sub(nbr_name, start = 38, end = 45)\n\n# we have just the dates stored as a vector, lets convert them to dates\nts_ndvi &lt;- as_date(ndvi_date,\n                   format = \"%Y%m%d\") # ts_dates\nts_nbr &lt;- as_date(nbr_date,\n                   format = \"%Y%m%d\")\n\n# now that we have some dates, lets open the files into a SpatRaster\nndvi_ts &lt;- rast(ndvi_list)\nnbr_ts &lt;- rast(nbr_list)\n\n# initiate an empty list for storing the composites\nndvi_composites_list &lt;- list()\n\nndvi_target_years &lt;- unique(year(ts_ndvi))\n\n# lets get looping!\nfor (year in seq_along(ndvi_target_years)) {\n  \n  # find the target years\n  trg_layers &lt;- which(year(ts_ndvi) == ndvi_target_years[year])\n  \n  # subset the target layers\n  ndvi_trg &lt;- subset(ndvi_ts, trg_layers)\n  \n  # calculate yearly average NDVI\n  ndvi_trg_avg &lt;- app(ndvi_trg, fun = mean, na.rm = TRUE)\n  \n  # assign name based on the year\n  names(ndvi_trg_avg) &lt;- paste0(\"\", ndvi_target_years[year])\n  \n  # add them to the list\n  ndvi_composites_list[[as.character(ndvi_target_years[year])]] &lt;- ndvi_trg_avg\n  \n}\n\nndvi_composite &lt;- rast(ndvi_composites_list)\n\n# lets do the same thing with NBR\nnbr_composite_list &lt;- list()\n\nnbr_target_years &lt;- unique(year(ts_nbr))\n\n# lets get looping!\nfor (y in 1:length(nbr_target_years)) {\n  \n  # find the target years\n  trg_layers &lt;- which(year(ts_nbr) == nbr_target_years[y])\n  \n  # subset the target layers\n  nbr_trg &lt;- subset(nbr_ts, trg_layers)\n  \n  # calculate yearly average NDVI\n  nbr_trg_avg &lt;- app(nbr_trg, fun = mean, na.rm = TRUE)\n  \n  # assign name based on the year\n  names(nbr_trg_avg) &lt;- paste0(\"\", nbr_target_years[y])\n  \n  # add them to the list\n  nbr_composite_list[[y]] &lt;- nbr_trg_avg\n  \n}\nnbr_composite &lt;- rast(nbr_composite_list)\n\nndvi_df &lt;- as.data.frame(ndvi_composite, xy = FALSE, na.rm = TRUE)\n\naverage_ndvi &lt;- colMeans(ndvi_df, na.rm = TRUE)\n\nndvi_summary &lt;- data.frame(\n  Year = as.numeric(gsub(\"Year_\", \"\", names(ndvi_composite))),\n  Average_NDVI = average_ndvi\n)\n\nggplot(ndvi_summary, aes(x = Year, y = Average_NDVI)) +\n  geom_point() +\n  geom_line(group = 1) +\n  labs(\n    title = \"Average NDVI of the Prouton Lakes Fire\",\n    x = \"Year\",\n    y = \"Average NDVI\"\n  ) +\n  theme_minimal() +\n  scale_x_continuous(breaks = seq(min(ndvi_summary$Year),\n                                  max(ndvi_summary$Year),\n                                  by = 1))\n\n\n\n\n\n\n\n\n\n\n\nStep 4 - Burn Severity Classification:\ndNBR (Delta NBR) is calculated using pre-fire (2015) and post-fire (2018) imagery. Burn severity is categorized into unburned, low, medium, and high severity using threshold values. Visual Output: Burn severity map with classification overlay.\n\n\nCode\n#| execute: true\n#| eval: true\n#| warning: false\n#| message: false\n#| code-fold: true\n#| code-summary: \"Show/Hide Code\"\n\n# set the pre and post fire nbr rasters\npre_fire_nbr &lt;- nbr_composite[[\"2015\"]]\npost_fire_nbr &lt;- nbr_composite[[\"2018\"]]\n\n\n\n# calculate the dNBR\ndNBR &lt;- pre_fire_nbr - post_fire_nbr\n\n# define the classification\nreclass_df &lt;- matrix(c(-0.2, 0.15, 1,\n                       0.15, 0.25, 2,\n                       0.25, 0.3, 3,\n                       0.3, 1.0, 4), ncol = 3, byrow = TRUE)\n\n\n# reclassify dNBR into the severity classes\nburn_severity &lt;- classify(dNBR, reclass_df)\n\n# set the burn severity for the matrix ids 1:4\nlevels(burn_severity) &lt;- data.frame(\n  id = 1:4,\n  label = c(\"unburned\", \"low severity\", \"medium severity\", \"high severity\"))\n\n# plot burn severity with the Prouton Fire Extent with legend\nplot(burn_severity, col = c(\"darkgreen\", \"yellow\", \"orange\", \"red\"),\n     main = \"Burn Severity of the Prouton Lakes Fire\")\nplot(prouton_fire_utm, add = TRUE, border = \"black\", lwd = 2, col = NA)\n\n\nWarning in plot.sf(prouton_fire_utm, add = TRUE, border = \"black\", lwd = 2, :\nignoring all but the first attribute\n\n\n\n\n\n\n\n\n\nCode\n# would have been a good place to look for black morels...\n\n\n\n\nStep 5 - Post-Fire Vegetation Recovery:\nConvert burn severity raster into polygons. Extract NDVI values for each burn severity class from 2018–2021. Visual Outputs: - Yearly NDVI maps from 2018 to 2021 (same value range 0–0.5). - Boxplots showing NDVI distribution by burn severity class over time.\n\n\nCode\n#| execute: true\n#| eval: true\n#| warning: false\n#| message: false\n#| code-fold: true\n#| code-summary: \"Show/Hide Code\"\n\n# first we need to convert the burn severity rasters to polygons\n# then load in the NDVI rasters from 2018, 2019, 2020 and 2021\n# and then extract the NDVI values by Burn Severity Class\n# then we can plot the yearly NDVI composites\n# finally create some boxplots showing the distribution of NDVI values per burn class by year\n\nburn_severity_masked &lt;- mask(burn_severity, vect(prouton_fire_utm)) # Mask to the fire extent\nburn_severity_polygons &lt;- as.polygons(burn_severity_masked, dissolve = TRUE)\n\nburn_severity_polygons$ID &lt;- 1:nrow(burn_severity_polygons)\n\nyears &lt;- c(2018, 2019, 2020, 2021)\nndvi_values &lt;- list()\n\nfor (year in years) {\n  ndvi_raster &lt;- ndvi_composites_list[[as.character(year)]]\n  \n  extracted_values &lt;- terra::extract(ndvi_raster, burn_severity_polygons,\n                                     xy = TRUE, ID = TRUE)\n  \n  extracted_values &lt;- merge(extracted_values, burn_severity_polygons, by = \"ID\",\n                            all.x = TRUE)\n  \n  colnames(extracted_values)[which(colnames(extracted_values) == as.character(year))] &lt;- \"value\"\n  \n  extracted_values$year &lt;- year\n  ndvi_values[[as.character(year)]] &lt;- extracted_values\n}\n\nndvi_all_years &lt;- do.call(rbind, ndvi_values)\n\nndvi_all_years$severity &lt;- factor(ndvi_all_years$label,\n                                  levels = c('unburned',\n                                             'low severity',\n                                             'medium severity',\n                                             'high severity'),\n                                  ordered = TRUE)\n\n\nfor (year in years) {\n  if (year %in% names(ndvi_composites_list)) {\n    plot(ndvi_composites_list[[as.character(year)]],\n         main = paste(\"Yearly NDVI Composite -\", year),\n         range = c(0, 0.5))\n    }\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(ndvi_all_years, aes(x = severity, y = value, fill = severity)) +\n  geom_boxplot() +\n  facet_wrap(~ year) +\n  scale_fill_manual(values = c(\"unburned\" = \"green\",\n                               \"low severity\" = \"yellow\",\n                               \"medium severity\" = \"orange\",\n                               \"high severity\" = \"red\"),\n                    name = \"Burn Severity\") +\n  labs(title = \"Distribution of NDVI Values by Burn Severity Class\",\n       x = \"Burn Severity Class\",\n       y = \"NDVI\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))"
  },
  {
    "objectID": "lidrforestmodel.html",
    "href": "lidrforestmodel.html",
    "title": "Estimating Forest Volume from Lidar Metrics",
    "section": "",
    "text": "Show/Hide Code\nlibrary(lidR)\nlibrary(terra)\nlibrary(tidyverse)"
  },
  {
    "objectID": "lidrforestmodel.html#load-in-necessary-packages",
    "href": "lidrforestmodel.html#load-in-necessary-packages",
    "title": "Estimating Forest Volume from Lidar Metrics",
    "section": "",
    "text": "Show/Hide Code\nlibrary(lidR)\nlibrary(terra)\nlibrary(tidyverse)"
  },
  {
    "objectID": "lidrforestmodel.html#read-in-data",
    "href": "lidrforestmodel.html#read-in-data",
    "title": "Estimating Forest Volume from Lidar Metrics",
    "section": "Read in Data",
    "text": "Read in Data\nSet working directory\nRead in the relevant .csv file\n\n\nShow/Hide Code\nwork_dir &lt;- \"~/GEM 521/Lab 2/Data\" # set file path to Lab 2 data\nsetwd(work_dir) # set working directory to that file path\n\nplot_table &lt;- read_csv(\"~/GEM 521/Lab 2/Data/Plots/Plot_Table.csv\") # .csv of the plot locations and \nmkrf_plot_metrics &lt;- read_csv(\"~/GEM 521/Lab 2/Data/Plots/mkrf_plot_metrics.csv\") # .csv metrics of the lidar point cloud\n\n#Add column to \"mkrf_plot_metrics' called Plot_ID (join key)\nmkrf_plot_metrics$Plot_ID = 1:20 # create a column with values 1-20 to join the plot_table to based on the ID.\n\n#Join 'Plot_Table' and 'MKRF_Plot_Metrics' into 'data_table'\ndata_table &lt;- plot_table %&gt;% \n  full_join(mkrf_plot_metrics) # join the plot_table to mkrf_plot in a seperate frame called data_table.\n\n\nNow we have a data frame with all the metrics from both the LiDAR and plot."
  },
  {
    "objectID": "lidrforestmodel.html#investigate-metrics",
    "href": "lidrforestmodel.html#investigate-metrics",
    "title": "Estimating Forest Volume from Lidar Metrics",
    "section": "Investigate Metrics",
    "text": "Investigate Metrics\n\n\nShow/Hide Code\n# list all the column names in data_table\ncolnames(data_table)\n\n\n [1] \"Plot_ID\"      \"X\"            \"Y\"            \"Net_Volume\"   \"...1\"        \n [6] \"zmax\"         \"zmean\"        \"zsd\"          \"zskew\"        \"zkurt\"       \n[11] \"zentropy\"     \"pzabovezmean\" \"pzabove2\"     \"zq5\"          \"zq10\"        \n[16] \"zq15\"         \"zq20\"         \"zq25\"         \"zq30\"         \"zq35\"        \n[21] \"zq40\"         \"zq45\"         \"zq50\"         \"zq55\"         \"zq60\"        \n[26] \"zq65\"         \"zq70\"         \"zq75\"         \"zq80\"         \"zq85\"        \n[31] \"zq90\"         \"zq95\"         \"zpcum1\"       \"zpcum2\"       \"zpcum3\"      \n[36] \"zpcum4\"       \"zpcum5\"       \"zpcum6\"       \"zpcum7\"       \"zpcum8\"      \n[41] \"zpcum9\"       \"itot\"         \"imax\"         \"imean\"        \"isd\"         \n[46] \"iskew\"        \"ikurt\"        \"ipground\"     \"ipcumzq10\"    \"ipcumzq30\"   \n[51] \"ipcumzq50\"    \"ipcumzq70\"    \"ipcumzq90\"    \"p1th\"         \"p2th\"        \n[56] \"p3th\"         \"p4th\"         \"p5th\"         \"pground\"      \"n\"           \n[61] \"area\"        \n\n\nWow look at all those metrics! We have our XY coordinates, net volume estimated out in the field at each of these plots, and all sorts of metrics that are automatically derived from the LiDAR data. Lets use these metrics to compute some statistics and eventually a model to estimate attributes of the forest.\nLets start with ploting the relationship between LiDAR metrics and volume.\n\n\nShow/Hide Code\npar(mfrow = c(2,3))  # Set layout to 2 rows and 3 columns\n\n# plot relationship between lidar metrics and volume.\nplot(Net_Volume ~ zq50, data = data_table) # ploting y as net_volume and x as zq50 (50th percentile height)\nplot(Net_Volume ~ zmax, data = data_table) # Maximum height of the LiDAR returns, representing the tallest point in the dataset\nplot(Net_Volume ~ zmean, data = data_table) # Mean height of all LiDAR returns, providing an average canopy or vegetation height.\nplot(Net_Volume ~ zkurt, data = data_table) # Kurtosis of the height distribution, describing the sharpness or flatness of the distribution curve.\nplot(Net_Volume ~ zsd, data = data_table) # Standard deviation of return heights, indicating variability in the canopy height structure.\nplot(Net_Volume ~ zpcum5, data = data_table) # cumulative percentage of returns below the 50th percentile (median height).\n\npar(mfrow = c(1,1))  # Reset to single plotting\n\n\n\n\n\nPlotted lidar metrics related to the volume"
  },
  {
    "objectID": "lidrforestmodel.html#comparing-models",
    "href": "lidrforestmodel.html#comparing-models",
    "title": "Estimating Forest Attributes from Lidar Metrics",
    "section": "Comparing Models",
    "text": "Comparing Models\nOkay, now that we have explored some of the metrics and how they relate to the measured volume of the trees within the plots, lets fit some linear models in those plots to see the relationship of those two variables and see if there is a significance between the two.\nFor this example, we will use the metrics zq90, pzabove2, zentropy and zskew in our model./ zq90 is the 90th percentile of lidar points, generally the upper canopy./ pzabove2 is the proportion of points above 2 meters, ei above the ground./ zentropy is the height distribution entropy, reflecting the complexity of vertical structure./ zsqew represents the skewness of height distribution showing whether the distribution is asymmetric./\nFirst lets start with a model with no variable so we can document how the model changes.\n\n\nCode\n#| execute: true\n#| eval: true\n#| echo: false\n#| warning: false\n#| message: false\n#| code-fold: true\n#| code-summary: \"Show/Hide Code\"\n\n## MODEL A\n\n# no variables in the model\nmodelA &lt;- lm(Net_Volume ~ 1, data = data_table)\n\n# added metrics to the model to predict volume\nadd1(modelA, ~ zq90 + pzabove2 + zentropy + zskew, test = 'F') # Compute all the single terms in the scope argument\n\n\nSingle term additions\n\nModel:\nNet_Volume ~ 1\n         Df Sum of Sq    RSS    AIC F value    Pr(&gt;F)    \n&lt;none&gt;                785465 213.57                      \nzq90      1    569694 215771 189.72 47.5249 1.905e-06 ***\npzabove2  1    236749 548716 208.39  7.7663  0.012176 *  \nzentropy  1     46746 738718 214.34  1.1390  0.299959    \nzskew     1    257447 528018 207.62  8.7763  0.008338 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCode\n# add zq90 into the model\nmodelA &lt;- lm(Net_Volume ~ zq90, data = data_table) # added zq90 to the model\n\nadd1(modelA, ~ zq90 + pzabove2 + zentropy + zskew, test = 'F')\n\n\nSingle term additions\n\nModel:\nNet_Volume ~ zq90\n         Df Sum of Sq    RSS    AIC F value  Pr(&gt;F)  \n&lt;none&gt;                215771 189.72                  \npzabove2  1     47061 168710 186.80  4.7420 0.04381 *\nzentropy  1     33616 182155 188.34  3.1373 0.09444 .\nzskew     1     25764 190007 189.18  2.3051 0.14732  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCode\n# add pzabove2 to model\nmodelA &lt;- lm(Net_Volume ~ zq90 + pzabove2, data = data_table) # adding pzabove2 to the current model\n\nadd1(modelA,~ zq90 + pzabove2 + zentropy + zskew, test = 'F')\n\n\nSingle term additions\n\nModel:\nNet_Volume ~ zq90 + pzabove2\n         Df Sum of Sq    RSS    AIC F value Pr(&gt;F)\n&lt;none&gt;                168710 186.80               \nzentropy  1     13759 154951 187.10  1.4207 0.2507\nzskew     1     13230 155481 187.17  1.3614 0.2604\n\n\nCode\n# final model summary statistics\nsummary(modelA)\n\n\n\nCall:\nlm(formula = Net_Volume ~ zq90 + pzabove2, data = data_table)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-153.548  -61.170   -5.696   40.334  277.285 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -1.618e+05  7.418e+04  -2.181   0.0435 *  \nzq90         1.744e+01  2.818e+00   6.188 9.92e-06 ***\npzabove2     1.616e+03  7.422e+02   2.178   0.0438 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 99.62 on 17 degrees of freedom\nMultiple R-squared:  0.7852,    Adjusted R-squared:  0.7599 \nF-statistic: 31.07 on 2 and 17 DF,  p-value: 2.1e-06"
  },
  {
    "objectID": "lidrforestmodel.html#model",
    "href": "lidrforestmodel.html#model",
    "title": "Estimating Forest Volume from Lidar Metrics",
    "section": "Model",
    "text": "Model\nNow that we have the lidar metrics, lets build a model that can estimate the Net_Volume.\n\n\nShow/Hide Code\n## MODEL A\nmodelA &lt;- lm(Net_Volume ~ zq25, data = data_table) \n\n#Get the output coefficients to our model\nmodelA$coefficients\n\n\n(Intercept)        zq25 \n -139.44524    24.95973 \n\n\nShow/Hide Code\n# final model summary statistics\nsummary(modelA)\n\n\n\nCall:\nlm(formula = Net_Volume ~ zq25, data = data_table)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-139.56  -52.90  -12.56   32.47  231.47 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -139.445     52.588  -2.652   0.0162 *  \nzq25          24.960      2.791   8.942 4.85e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 89.54 on 18 degrees of freedom\nMultiple R-squared:  0.8163,    Adjusted R-squared:  0.8061 \nF-statistic: 79.97 on 1 and 18 DF,  p-value: 4.846e-08\n\n\nNow that we have developed a model, we will apply the model over our entire study area To do this, we must calculate LiDAR metrics over the entire study area on a grid. If we use ModelA, we have to calculate the 25th percentile.\nWe will use pixel_metrics function in lidR and the app function in terra.\nLets calculate grid_metrics for all MKRF. We need to calculate the zq25 metric for each pixel because depending on the template it can be for each pixel of a raster (area-based approach), or each polygon, or each segmented tree, or on the whole point cloud. In our case we will be doing every pixel. For the time sake, the spatial resolution will be set to 10 meters.\n\n\nShow/Hide Code\n# Calculate grid_metrics for all MKRF \n# Create LAScatalog of filtered, normalized tiles with points 2 m - 65 m \nnorm_cat_mkrf &lt;- readLAScatalog(\"~/GEM 521/Lab 2/Data/Normalized\") # read in Normalized LAS file from Lab2\nopt_filter(norm_cat_mkrf) &lt;- '-keep_z_above 2 -drop_z_above 65' # remove points above 65m and below 2 meters\n\n#Calculate grid metrics of mean Z at 10 m resolution for entire study area\npixel_metrics_mkrf &lt;- pixel_metrics(norm_cat_mkrf, .stdmetrics_z, 10) \nplot(pixel_metrics_mkrf)\n\n\n\n\n\nAll of the MKRF pixel metrics\n\n\nLook at all the rasters we have created. We can plot individual rasters using variations of plot(pixel_metrics_mkrf). For this model, we want to extract the zq25 raster from this SpatRaster. We can do that by using the subset function in the terra package. Let’s plot the extracted SpatRaster to confirm it was successful.\nAfter extracting the 25th percentile for each pixel across the study area we can apply Model 2. To do this, we first need to write a function that will apply this equation to all pixels in zq25_mkrf. The function we will create is based on the coefficients derived from Model 2. And lets apply that function to the entire MKRF study area.\n\n\nShow/Hide Code\nzq25_mkrf &lt;- terra::subset(pixel_metrics_mkrf, \"zq25\") # assign the subset zq25 metric to a variable\n\n# function based on Model2 (ModelB) coefficients\nf &lt;- function(x) {\n  24.96* x -139.45\n}\n\n#Apply function to raster\nnet_volume_mkrf &lt;- terra::app(zq25_mkrf, f) # apply function f to the zq25 metric across the entire area\nplot(net_volume_mkrf,\n     main = 'Estimated Volume using ModelB',) # plot, in 2D, the estimated volume based on our model\n\n\n\n\n\n2D visualization of the Volume based on the model"
  },
  {
    "objectID": "lidr3dmodeling.html",
    "href": "lidr3dmodeling.html",
    "title": "Geospatial LiDAR Analysis in R",
    "section": "",
    "text": "In this report, I focus on LiDAR data processing, forest attribute estimation, and model development using R. I begin by loading the necessary libraries, including lidR, terra, tidyverse, MASS, and corrplot, to facilitate LiDAR data processing, geospatial raster operations, statistical modeling, and visualization.\nTo prepare the data, I read LiDAR tiles (.las files) into a LAScatalog object, filter and normalize the point cloud using DEM-based normalization, and extract individual plots while ensuring the removal of duplicate points. I then generate Digital Elevation Models (DEM) and Canopy Height Models (CHM) to provide spatial context for further analysis.\nFollowing this, I compute standard LiDAR cloud metrics such as mean height (zmean), maximum height (zmax), median height (zq50), and standard deviation (zsd). These metrics allow me to analyze forest structure across individual plots and the entire study area. I explore relationships between these LiDAR metrics and vegetation attributes, visualizing correlations and patterns.\nTo estimate forest attributes, I develop linear regression models for Above-Ground Biomass (AGB) and Dominant Height (DH) based on LiDAR metrics. Using stepwise variable selection, I optimize the model to improve predictive accuracy. I evaluate model performance by analyzing correlation matrices and comparing predicted values against measured data.\nOnce the best-fit models are identified, I apply them to LiDAR-derived rasters to generate spatial predictions of AGB and DH across the study area. By performing raster algebra, I compute differences between predicted and actual values, assessing the model’s effectiveness. Finally, I export the processed data and model outputs as .csv files for further analysis.\nThroughout this process, I use various visualization techniques, including plotting LiDAR-derived metrics, DEMs, CHMs, and model results. I also generate correlation matrices to examine relationships between different variables and validate the accuracy of my models.\nOverall, this report presents a complete LiDAR processing pipeline, from raw .las files to model-based forest attribute estimation. By integrating spatial data analysis, raster computation, and statistical modeling, I develop a data-driven approach to assessing forest structure and biomass distribution."
  },
  {
    "objectID": "lidr3dmodeling.html#load-in-necessary-packages",
    "href": "lidr3dmodeling.html#load-in-necessary-packages",
    "title": "Geospatial LiDAR Analysis in R",
    "section": "Load in necessary packages",
    "text": "Load in necessary packages\n\n\nShow/Hide Code\nlibrary(lidR)\nlibrary(terra)\nlibrary(tidyverse)\nlibrary(rgl)\nlibrary(htmlwidgets)"
  },
  {
    "objectID": "lidr3dmodeling.html#set-wd-and-load-in-some-data",
    "href": "lidr3dmodeling.html#set-wd-and-load-in-some-data",
    "title": "Geospatial LiDAR Analysis in R",
    "section": "Set WD and Load in some Data",
    "text": "Set WD and Load in some Data\n\n\nShow/Hide Code\n# set working directory\nsetwd(\"C:/Users/lorsigno.stu/OneDrive - UBC/Desktop/Documents/GEM 521/Lab 4/L4_Data\")\nwd &lt;- \"C:/Users/lorsigno.stu/OneDrive - UBC/Desktop/Documents/GEM 521/Lab 4/L4_Data\"   # create a variable string working directory\n\n# load in the RGB image and explore the structure\nrgb_afrf &lt;- rast(\"~/GEM 521/Lab 4/L4_Data/Aerial_Photo/AFRF_Aerial_Photo.tif\") # read in the .tif aerial photo using the rast function\nstr(rgb_afrf)         # display the structure of the SpatRaster\n\n\nS4 class 'SpatRaster' [package \"terra\"]\n\n\nShow/Hide Code\nplotRGB(rgb_afrf)     # plot the SpatRaster as an RGB image"
  },
  {
    "objectID": "lidr3dmodeling.html#set-working-directory",
    "href": "lidr3dmodeling.html#set-working-directory",
    "title": "Geospatial LiDAR Analysis in R",
    "section": "Set Working Directory",
    "text": "Set Working Directory\n\n\nShow/Hide Code\n# set working directory\nsetwd(\"C:/Users/lorsigno.stu/OneDrive - UBC/Desktop/Documents/GEM 521/Lab 4/L4_Data\")\nwd &lt;- \"C:/Users/lorsigno.stu/OneDrive - UBC/Desktop/Documents/GEM 521/Lab 4/L4_Data\"   # create a variable string working directory\n\n# load in the RGB image and explore the structure\nrgb_afrf &lt;- rast(\"~/GEM 521/Lab 4/L4_Data/Aerial_Photo/AFRF_Aerial_Photo.tif\") # read in the .tif aerial photo using the rast function\nstr(rgb_afrf)         # display the structure of the SpatRaster\nplotRGB(rgb_afrf)     # plot the SpatRaster as an RGB image\n\n\n\n\n\nAerial Image of the true color composite for Alex Fraser Research Forest"
  },
  {
    "objectID": "lidr3dmodeling.html#plot-2d-digital-elevation-model",
    "href": "lidr3dmodeling.html#plot-2d-digital-elevation-model",
    "title": "Geospatial LiDAR Analysis in R",
    "section": "Plot 2D Digital Elevation Model",
    "text": "Plot 2D Digital Elevation Model\n\n\nShow/Hide Code\n#Create LAScatalog object from afrf las tiles\ncat_afrf &lt;- readLAScatalog(\"~/GEM 521/Lab 4/L4_Data/LAS\")\n\n#Set the output directory for the filtered .las data\nopt_output_files(cat_afrf) &lt;- paste(wd, \"/Filtered/filtered_afrf_{ID}\", sep = \"\")\n\n#read filtered .las into LAScatalog\nfiltered_cat_afrf &lt;- readLAScatalog(\"~/GEM 521/Lab 4/L4_Data/Filtered\")\n\n#Create DEM\ndem_allLAS_afrf &lt;- rasterize_terrain(filtered_cat_afrf, 2, tin())\n\n#Create color palette\ncol_1 &lt;- height.colors(50) \n\n#Plot DEM using color palette\nplot(dem_allLAS_afrf, col = col_1,\n     main = 'DEM of Alex Fraser Research Forest') #plot in 2D\n\n\n\n\n\nDigital Elevation Model (DEM) of Alex Fraser Research Forest"
  },
  {
    "objectID": "lidr3dmodeling.html#create-and-plot-chm",
    "href": "lidr3dmodeling.html#create-and-plot-chm",
    "title": "Geospatial LiDAR Analysis in R",
    "section": "Create and Plot CHM",
    "text": "Create and Plot CHM\n\n\nShow/Hide Code\n#read normalized las into catalog to continue processing\nnorm_cat_afrf &lt;- readLAScatalog(\"~/GEM 521/Lab 4/L4_Data/Normalized\")\n\n#add LAScatalog enginge option to filter undersired data points\nopt_filter(norm_cat_afrf) &lt;- '-drop_z_below 0 -drop_z_above 55'\n\n#Create CHM for all normalized afrf Tiles\nchm_afrf &lt;- rasterize_canopy(norm_cat_afrf, 2, p2r()) \nplot(chm_afrf, col = col_1,\n     main = 'Canopy Height Model of Alex Fraser Research Forest') #plot in 2D\n\n\n\n\n\nCanopy Height Model (CHM) of Alex Fraser Research Forest"
  },
  {
    "objectID": "lidr3dmodeling.html#plot-extraction",
    "href": "lidr3dmodeling.html#plot-extraction",
    "title": "Geospatial LiDAR Analysis in R",
    "section": "Plot Extraction",
    "text": "Plot Extraction\nWe extract the field plots and calculate some cloud metrics\nThen combine the plots and cloud metrics into one data table.\n\n\nShow/Hide Code\n# read in the .csv table with field data\nafrf_plot_table &lt;- read.csv(\"~/GEM 521/Lab 4/L4_Data/Plots/Plot_Table.csv\")\n\n# define the size if the plot radius\nradius &lt;- 10\n\n#for loop to extract multiple plots\nfor(i in 1:nrow(afrf_plot_table)){ #run the loop until i = the number of rows in 'plot_table' (20)\n  plot_cent &lt;- c(afrf_plot_table$X[i], afrf_plot_table$Y[i]) #extract plot center\n  plot_las &lt;- clip_circle(norm_cat_afrf, plot_cent[1], plot_cent[2], radius) #clip plot from norm_cat_las\n  output_file &lt;- paste(\"~/GEM 521/Lab 4/L4_Data/Plots/afrf_Plot_\", i, \".las\", sep = \"\") #output directory as string\n  writeLAS(assign(paste(\"afrf_Plot_\", i, sep = \"\"), plot_las), output_file) #write'afrf_Plot_i' to output dir.\n}\n\n#check a plot\nplot1 &lt;- readLAS(\"~/GEM 521/Lab 4/L4_Data/Plots/afrf_Plot_1.las\")\nplot(plot1)\n\n#Calculate cloud metrics for all plots\n#create empty dataframe\nafrf_plot_metrics_2 &lt;- data.frame() \n\n#For loop to calculate cloud metrics for all plots and add them to 'afrf_cloud_metrics'\nfor(i in 1:nrow(afrf_plot_table)){ #for loop == number of rows in plot_table (20)\n  plot &lt;- readLAS(paste(\"~/GEM 521/Lab 4/L4_Data/Plots/afrf_Plot_\", i, \".las\", sep= \"\"), filter = '-drop_z_below 0 -drop_z_above 55')\n  metrics &lt;- cloud_metrics(plot, .stdmetrics) #compute standard metrics\n  afrf_plot_metrics_2 &lt;- rbind(afrf_plot_metrics_2, metrics) #add the new 'metrics' to 'afrf_cloud_metrics'\n}\n\n#Add column to \"afrf_plot_metrics' called Plot_ID (join key)\nafrf_plot_metrics_2$Plot_ID = 1:38 # create a column with values 1-20 to join the plot_table to based on the ID.\n\n#Join 'Plot_Table' and 'afrf_Plot_Metrics' into 'data_table'\ndata_table &lt;- afrf_plot_table %&gt;% \n  full_join(afrf_plot_metrics_2) # join the plot_table to afrf_plot in a seperate frame called data_table.\n\n\n\n\nShow/Hide Code\n#check a plot\nplot1 &lt;- readLAS(\"~/GEM 521/Lab 4/L4_Data/Plots/afrf_Plot_1.las\")\nplot(plot1)\n\n# Open a 3D rendering window for Li et al.2012\nopen3d()\nplot(plot1)\nrglwidget_obj_plot1 &lt;- rglwidget()\nsaveWidget(rglwidget_obj_plot1, file = \"interactive_plot1.html\", selfcontained = TRUE)"
  },
  {
    "objectID": "lidr3dmodeling.html#develop-statistical-model-for-agb-estimation",
    "href": "lidr3dmodeling.html#develop-statistical-model-for-agb-estimation",
    "title": "Geospatial LiDAR Analysis in R",
    "section": "Develop Statistical Model for AGB Estimation",
    "text": "Develop Statistical Model for AGB Estimation\n\n\nShow/Hide Code\nmodelAGB &lt;- lm(Total_AGB ~ zmean + zsd + zmax, data = data_table)\n\n# plot predicted model vs measured volume\nplot(Total_AGB ~ modelAGB$fitted, \n     data = data_table, \n     xlab = 'Predicted', \n     ylab = 'Measured',\n     main = 'ModelAGB Predicting AGB (kg/ha)')\nabline(0, 1) # fit a one to one line of best fit\n\n\n\n\n\nPlot of the predicted vs measured AGB using modelAGB\n\n\n\n\nShow/Hide Code\n# define the metrics we want\nf &lt;- function(z) {\n  list(\n    zmean = mean(z), \n    zsd = sd(z),\n    zmax = max(z))\n}\n\n#Calculate pixel metrics for entire study area\npixel_metrics_afrf &lt;- pixel_metrics(norm_cat_afrf, func = ~f(Z), 10)\n\n# subset zq95\nzmean_afrf &lt;- terra::subset(pixel_metrics_afrf, \"zmean\")\n\n# subset zsd\nzsd_afrf &lt;- terra::subset(pixel_metrics_afrf, \"zsd\")\n\n# subset zmax\nzmax_afrd &lt;- terra::subset(pixel_metrics_afrf, \"zmax\")\n\n# stack raster\nraster_stack_AGB &lt;- c(zmean_afrf, zsd_afrf, zmax_afrd)\n\n# create function from model\nAGB_func &lt;- function(x){\n  30539.090*x[1] - 25978.149*x[2] + 5336.517*x[3] - 44006.100\n}\n\n# apply function to raster\nmodel_AGB_predictor &lt;- terra::app(raster_stack_AGB, AGB_func)\nplot(model_AGB_predictor,\n     main = 'Estimated AGB using ModelAGB (kg/ha)')\n\n\n\n\n\nFinal AGB Predictions using modelAGB"
  },
  {
    "objectID": "itd.html",
    "href": "itd.html",
    "title": "Individual Tree Detection Methods",
    "section": "",
    "text": "In this report, I preprocess and analyze LiDAR data using R and various geospatial libraries such as lidR, terra, and tidyverse. I begin by setting up my working directory and loading the required packages. Then, I work with filtered LiDAR point clouds by reading a .las dataset into an LAScatalog object, summarizing the data, and visualizing it.\nNext, I generate a Digital Elevation Model (DEM) from the LiDAR point cloud using a TIN (Triangulated Irregular Network) interpolation approach. I apply a color palette to enhance the visualization and plot the DEM to inspect elevation variations across the study area.\nIn addition to generating a Digital Elevation Model (DEM), I perform individual tree detection (ITD) using the lidR package. Specifically, I implement the Li et al. (2012) and Dalponte et al. (2016) methods to identify and segment individual trees from the LiDAR point cloud.\nThe Li et al. (2012) method relies on a local maxima filtering approach applied to a Canopy Height Model (CHM). I smooth the CHM to reduce noise and extract tree tops using a variable window size, ensuring better accuracy in detecting dominant trees and visualize the detected trees and assess the segmentation results.\nThe Dalponte et al. (2016) method incorporates a marker-controlled watershed segmentation, which refines tree crown delineation based on spectral and structural characteristics. Using a Gaussian smoothing filter and use the tree segmentation function in lidR to identify tree crowns, I compare the results with the Li (2012) method to evaluate their effectiveness. Throughout this process, I fine-tune parameters such as smoothing intensity, window size, and threshold values to optimize tree detection accuracy. The results allow me to assess the effectiveness of these ITD methods in forest structure analysis and remote sensing applications."
  },
  {
    "objectID": "itd.html#load-packages",
    "href": "itd.html#load-packages",
    "title": "Individual Tree Detection Methods",
    "section": "Load Packages",
    "text": "Load Packages\n\n\nShow/Hide Code\nlibrary(lidR)\nlibrary(terra)\nlibrary(tidyverse)\nlibrary(rgl)\nlibrary(htmlwidgets)\n\n# set working directory\nsetwd(\"~/GEM 521/Lab 5/Data\")\nwd &lt;- \"~/GEM 521/Lab 5/Data\"   # create a variable string working directory"
  },
  {
    "objectID": "itd.html#create-dem",
    "href": "itd.html#create-dem",
    "title": "Individual Tree Detection Methods",
    "section": "Create DEM",
    "text": "Create DEM\nThe first thing we need to produce is a Digital Elevation Model. This is so we can normalize the LiDAR data to an accurate elevation so we can extract just the points we need to create our models.\n\n\nShow/Hide Code\n# read filtered .las into LAScatalog\nfiltered_cat_mkrf &lt;- readLAScatalog(\"~/GEM 521/Lab 5/Data/Filtered\")\nsummary(filtered_cat_mkrf)\nplot(filtered_cat_mkrf)\n\n# create color palette\ncol_1 &lt;- height.colors(50)\n\n# create DEM\ndem_allLAS_mkrf &lt;- rasterize_terrain(filtered_cat_mkrf, 2, tin())\n\n# plot DEM using color palette\nplot(dem_allLAS_mkrf, col = col_1,\n     main = 'DEM of Alex Fraser Research Forest') #plot in 2D\n\n\n\n\n\nDEM of Malcom Knapp Research Forest"
  },
  {
    "objectID": "itd.html#create-chm",
    "href": "itd.html#create-chm",
    "title": "Individual Tree Detection Methods",
    "section": "Create CHM",
    "text": "Create CHM\nAfter we have the DEM, we can normalize the LAS catalog to the DEM we just created. Once normalize, we can then rasterize the LAS in order to plot the Canopy Height Model (CHM). This is a model that shows the height, or tops, of the trees.\n\n\nShow/Hide Code\n# read normalized las into catalog to continue processing\nnorm_cat_mkrf &lt;- readLAScatalog(\"~/GEM 521/Lab 5/Data/Normalized\")\n\n# add LAScatalog enginge option to filter undersired data points\nopt_filter(norm_cat_mkrf) &lt;- '-drop_z_below 0 -drop_z_above 65'\n\n# ensure the entire study area was processed\nplot(norm_cat_mkrf)\nsummary(norm_cat_mkrf)\n\n# plot the CHM just to make sure things look good\nplot(chm_mkrf, col = col_1,\n     main = 'Canopy Height Model of Alex Fraser Research Forest') # plot in 2D\n\n\n\n\n\nCanopy Height Model of Malcom Knapp Research Forest"
  },
  {
    "objectID": "itd.html#extract-plots",
    "href": "itd.html#extract-plots",
    "title": "Individual Tree Detection Methods",
    "section": "Extract Plots",
    "text": "Extract Plots\nNow that we have our normalized CHM of the study site, we can start to work on the individual tree detection. First we need to access the field data where the plot data was recorded. There were four plots that were extracted, with a radius of 154 meters that were extracted, using given coordinates based on the .csv. After extraction, we can read in the plots as LAS files to compute metrics as well as visualize them. We can see one of the plots visualized in 3D below.\n\n\nShow/Hide Code\n# read in csv file with plot locations\ntree_plot_table &lt;- read.csv(\"~/GEM 521/Lab 5/Data/Lab5_Plots.csv\")\n\n# set the radius for the plots\nradius &lt;- 154\n\n# for loop to extract the 4 plots needed for the tree detection\nfor(i in 1:nrow(tree_plot_table)){                                              # run the loop until i = the number of rows in 'plot_table' (4)\n  plot_cent &lt;- c(tree_plot_table$X[i], tree_plot_table$Y[i])                    # extract plot center\n  plot_las &lt;- clip_circle(norm_cat_mkrf, plot_cent[1], plot_cent[2], radius)    # clip plot from norm_cat_mkrf\n  output_file &lt;- paste(\"~/GEM 521/Lab 5/Data/Plots/Plot_\", i, \".las\", sep = \"\") # output directory as string\n  writeLAS(assign(paste(\"Plot_\", i, sep = \"\"), plot_las), output_file)          # write'TD_Plot_i' to output dir.\n}\n\n# read in the plots\nplot_1 &lt;- readLAS(\"~/GEM 521/Lab 5/Data/Plots/Plot_1.las\")\nplot_2 &lt;- readLAS(\"~/GEM 521/Lab 5/Data/Plots/Plot_2.las\")\nplot_3 &lt;- readLAS(\"~/GEM 521/Lab 5/Data/Plots/Plot_3.las\")\nplot_4 &lt;- readLAS(\"~/GEM 521/Lab 5/Data/Plots/Plot_4.las\")\n\n# visualize the plots and compare\nplot(plot_1)"
  },
  {
    "objectID": "itd.html#visualize",
    "href": "itd.html#visualize",
    "title": "Individual Tree Detection Methods",
    "section": "Visualize",
    "text": "Visualize\nBelow is a interactive visualization of one of the extracted plots from Malcom Knapp Research Forest. We will be detecting each one of these trees using two different tree detection methods.\n\n\nShow/Hide Code\n# Read in one of the plots\nplot_1 &lt;- readLAS(\"~/GEM 521/Lab 5/Data/Plots/Plot_1.las\")\n\n# Open a 3D rendering window for Li et al.2012\nopen3d()\nplot(plot_1)\nrglwidget_obj_plot_1 &lt;- rglwidget()\nsaveWidget(rglwidget_obj_plot_1, file = \"interactive_plot_1.html\", selfcontained = TRUE)"
  },
  {
    "objectID": "itd.html#itd-using-li-2012",
    "href": "itd.html#itd-using-li-2012",
    "title": "Individual Tree Detection Methods",
    "section": "ITD using Li (2012)",
    "text": "ITD using Li (2012)\nPretty neat! Now lets actually detect some trees with some built in tree detection algorithms. The first one we are going to use is built by li(2012). We are going to do this for all of the 4 plots we extracted.\n\n\nShow/Hide Code\n# segment each plot using li2012 algorithm\nplot_1_seg &lt;- segment_trees(plot_1, li2012(dt1 = 1.5, dt2 = 2, R = 2, Zu = 15, hmin = 2, speed_up = 10))\nplot_2_seg &lt;- segment_trees(plot_2, li2012(dt1 = 1.5, dt2 = 2, R = 2, Zu = 15, hmin = 2, speed_up = 10))\nplot_3_seg &lt;- segment_trees(plot_3, li2012(dt1 = 1.5, dt2 = 2, R = 2, Zu = 15, hmin = 2, speed_up = 10))\nplot_4_seg &lt;- segment_trees(plot_4, li2012(dt1 = 1.5, dt2 = 2, R = 2, Zu = 15, hmin = 2, speed_up = 10))\n\n# write each segmented plot to a .las file\nwriteLAS(plot_1_seg, \"plot_1_seg.las\")\nwriteLAS(plot_2_seg, \"plot_2_seg.las\")\nwriteLAS(plot_3_seg, \"plot_3_seg.las\")\nwriteLAS(plot_4_seg, \"plot_4_seg.las\")\n\n# visualize each plot\nplot(plot_1_seg, color = 'treeID')\nplot(plot_2_seg, color = 'treeID')\nplot(plot_3_seg, color = 'treeID')\nplot(plot_4_seg, color = 'treeID')\n\n\nLets see how it did!\n\n\nShow/Hide Code\n# segment each plot using li2012 algorithm\nplot_1_seg &lt;- segment_trees(plot_1, li2012(dt1 = 1.5, dt2 = 2, R = 2, Zu = 15, hmin = 2, speed_up = 10))\n\n# Open a 3D rendering window for Li et al.2012\nopen3d()\nplot(plot_1_seg, color = \"treeID\")\nrglwidget_plot1_li &lt;- rglwidget()\nsaveWidget(rglwidget_plot1_li, file = \"interactive_plot1_li.html\", selfcontained = TRUE)"
  },
  {
    "objectID": "itd.html#itd-using-dalponte-coomes-2016",
    "href": "itd.html#itd-using-dalponte-coomes-2016",
    "title": "Individual Tree Detection Methods",
    "section": "ITD using Dalponte Coomes (2016)",
    "text": "ITD using Dalponte Coomes (2016)\nLets use another algorith, created by Dalponte Coomes (2016) this time. Same thing, we are going to focus on the first plot we extracted.\n\n\nShow/Hide Code\n# create chm for plot 1 with given pitfree values\nchm_plot1 &lt;- rasterize_canopy(plot_1, res = 0.5, \n                         algorithm = pitfree(c(0, 10, 20, 30), subcircle = 0.2))\n\n# use the locate_trees to 'locate the trees' using default values\nplot1_lmf &lt;- locate_trees(chm_plot1, lmf(ws = 5, hmin = 2, shape = \"circular\", ws_args = \"z\"))\n\n# plot in 2D the CHM of plot 1 and the located trees using the lmf algorithm\nplot(chm_plot1, main = \"dalponte2016 Tree Detection of Plot 1\")\nplot(plot1_lmf, add = TRUE, col = 'pink')\n\n\n\n\n\n2D representation of the Dalponte Coomes (2016) individual tree detection\n\n\nHmm… Didn’t seem too do too well… Lets take a look at it in 3D to see whats up.\n\n\nShow/Hide Code\n# create chm for plot 1 with given pitfree values\nchm_plot1 &lt;- rasterize_canopy(plot_1, res = 0.5, \n                         algorithm = pitfree(c(0, 10, 20, 30), subcircle = 0.2))\n\n# use the locate_trees to 'locate the trees' using default values\nplot1_lmf &lt;- locate_trees(chm_plot1, lmf(ws = 5, hmin = 2, shape = \"circular\", ws_args = \"z\"))\n\n# segment point cloud using dalponte2016\nplot1_dalp &lt;- segment_trees(plot_1, dalponte2016(chm_plot1, treetops = plot1_lmf))\n\n# Open a 3D rendering window for Li et al.2012\nopen3d()\nplot(plot1_dalp, color = \"treeID\")\nrglwidget_plot1_dalp &lt;- rglwidget()\nsaveWidget(rglwidget_plot1_dalp, file = \"interactive_plot1_dalp.html\", selfcontained = TRUE)\n\n\n\n\n\n\nCan you find out why, in the 2D tree detection plot, that the algorithm detected trees where there seemingly were not any?"
  },
  {
    "objectID": "cv2.html#lead-field-technician-at-aquacoustic-remote-technologies",
    "href": "cv2.html#lead-field-technician-at-aquacoustic-remote-technologies",
    "title": "CV",
    "section": "Lead Field Technician at AquaCoustic Remote Technologies",
    "text": "Lead Field Technician at AquaCoustic Remote Technologies\n(2020-2024)\n\nSetting up, testing and deploying robotic platforms including crawlers and pontoons of different sizes, ROVs, and submersibles to collect video, sonar, laser and additional sensor data\nInspections, cleaning and maintenance of robotic platforms, fiber optic winches, vehicles and other warehouse equipment\nHands-on troubleshooting, assembly and repair of equipment inhouse and in the field\nKeeping accurate daily logs and other detailed inspections sheets and paperwork\nUtility vehicle interior designing, building and maintenance, including office space, electronics and winches\nNightwork and all-weather type field work, working independently, within a team capacity and with other contractors\nProcessing sonar, laser and video data using custom and sourced software"
  },
  {
    "objectID": "cv2.html#publications",
    "href": "cv2.html#publications",
    "title": "CV",
    "section": "Publications",
    "text": "Publications\nReports:\n\nFCOR 599 OECM Land Designation in Howe Sound Biosphere.\n\n\nOther Experiences\nVolunteering:\n\nStanley Park Ecology DIRT (Dedicated Invasive Removal Team)\nAME Roundup\nChuckleBerry Organic Farm"
  },
  {
    "objectID": "cv2.html#skills-and-certificates",
    "href": "cv2.html#skills-and-certificates",
    "title": "CV",
    "section": "",
    "text": "ArcGIS Pro & QGIS\nArcGIS Model Builder and Experience Builder\nFME (Feature Manipulation Engine) for Workflow Automation\nPostgreSQL for Spatial Database Management\nENVI for Remote Sensing Image Analysis\nCartography and Map Design\n\n\n\n\n\nArcGIS Pro, QGIS, ENVI, FME\nPostgreSQL\nPython, R, SQL\nAdobe Illustrator, Photoshop, SketchUp, Procreate\nMicrosoft Office (Excel, Word, PowerPoint)\n\n\n\n\n\nClass 5 Drivers License\nWorkSafeBC\nWHMIS Certified\nFirst Aid (WFA 40 Hour)\nBoating License\nDrone License"
  },
  {
    "objectID": "cv2.html#education",
    "href": "cv2.html#education",
    "title": "CV",
    "section": "Education",
    "text": "Education\n\n\nMasters in Geomatics for Environmental Management UBC Vancouver (2024-2025)\nB.Sc Earth and Environmental Science UBC Okanagan (2016-2019)"
  },
  {
    "objectID": "cv2.html#professional-experience",
    "href": "cv2.html#professional-experience",
    "title": "CV",
    "section": "Professional Experience",
    "text": "Professional Experience\n\nLead Field Technician at AquaCoustic Remote Technologies\n(2020-2024)\n\nSetting up, testing and deploying robotic platforms including crawlers and pontoons of different sizes, ROVs, and submersibles to collect video, sonar, laser and additional sensor data\nInspections, cleaning and maintenance of robotic platforms, fiber optic winches, vehicles and other warehouse equipment\nHands-on troubleshooting, assembly and repair of equipment inhouse and in the field\nKeeping accurate daily logs and other detailed inspections sheets and paperwork\nUtility vehicle interior designing, building and maintenance, including office space, electronics and winches\nNightwork and all-weather type field work, working independently, within a team capacity and with other contractors\nProcessing sonar, laser and video data using custom and sourced software\n\n\n\n\nCo-Founder Canopy Campers Van Rentals\n(2023 - Present)\n\nModelling van interiors using SketchUp for fully off-grid 4-person camper vans\nCustom woodworking and metal working for modular van camping set-ups\nClient interaction & conlfict resolution online and in person, cash and POI transactions\nVehicle maintenance and repairs, regular cleaning, organizing and restocking\n\n\n\nVisitors Service Attendant for Banff National Park\n(Summers 2018 and 2019)\n\nPublic outreach: Providing orientation, information and suggestions to visitors in Banff National Park\nEnforcing Park closures, restrictions and regulations with compliance and ticketing\nEmployee uniform and accessories inventory, distribution and organizing\nUpdating Dispatch and Resource Conservation on wildlife encounters, fires and other public safety concerns\nCustomer service - Handling cash and credit POS transactions in any weather situation quickly and effectively\n\n\n\nPublications\nReports:\n\nFCOR 599 OECM Land Designation in Howe Sound Biosphere.\n\n\n\nOther Experiences\nVolunteering:\n\nStanley Park Ecology DIRT (Dedicated Invasive Removal Team)\nAME Roundup\nChuckleBerry Organic Farm"
  },
  {
    "objectID": "burnseverity2.html",
    "href": "burnseverity2.html",
    "title": "Burn Severity Analysis of Prouton Lakes Fire",
    "section": "",
    "text": "In this lab, I analyze the burn severity of the 2017 Prouton Lakes wildfire using Landsat 8 time-series data (2013–2021) to assess vegetation loss and post-fire recovery. By computing NDVI and NBR indices, I quantify fire impact and track regrowth over time. Additionally, I classify burn severity using dNBR thresholds and examine recovery patterns across severity classes.\nThis analysis is valuable for forest management, wildfire risk assessment, and ecological monitoring, helping decision-makers understand fire dynamics and long-term landscape recovery. Similar methodologies can be applied to other wildfire-prone regions, post-disaster assessments (e.g., volcanic eruptions, floods), and habitat restoration projects, providing critical insights for conservation and land-use planning.\n\nLoad in necessary packages\n\n\nCode\n#| execute: false\n#| eval: false\n#| echo: false\n#| code-fold: true\n#| code-summary: \"Show/Hide Code\"\n\n#Attach packages\n\nlibrary(terra)\nlibrary(sf)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(readr)\nlibrary(stringr)\nlibrary(lubridate)\nlibrary(tidyr)\n\n\n\n\nStep 1 - Historical Fire Analysis:\nIdentify total fires and area burned in 2017. Extract fire IDs and areas for the three largest fires in 2017. Compute the area burned by the Prouton Lakes fire (C30870). Visual Output: Bar plot of total area burned per year in BC, colored by fire cause.\n\n\nCode\n#| execute: false\n#| eval: false\n#| echo: false\n#| code-fold: true\n#| code-summary: \"Show/Hide Code\"\n\n#read in the shape file\nfires &lt;- st_read(\"C:/Users/lorsigno.stu/OneDrive - UBC/Desktop/Documents/GEM 520/Lab 8/lab8_finalDemonstrationOfRSkills-assign/data/PROT_HISTORICAL_FIRE_POLYS_SP/H_FIRE_PLY_polygon.shp\", quiet = TRUE)\n\n#filter out the correct year 2017 into another df\nfires_2017 &lt;- fires %&gt;% filter(FIRE_YEAR == 2017)\n#count number of fires within that year\nnumber_fires_2017 &lt;- nrow(fires_2017)\n#calculate the area, in ha, of the fires in 2017\narea_ha_fires_2017 &lt;- sum(fires_2017$SIZE_HA, na.rm = TRUE) #there are no NA values, but I would assume that theres been some preprocessing of this dataset\n\n#find the area of the three largest fires in 2017\n#sort the SIZE_HA into ascending order, extract 3 top rows\nlargest_fires_2017 &lt;- fires_2017 %&gt;% \n  arrange(desc(SIZE_HA)) %&gt;% \n  slice(1:3)\n\n#prouton lake fire is polygon C30870\nprouton_fire &lt;- fires %&gt;% filter(FIRE_NO == 'C30870')\nprouton_fire_area &lt;- prouton_fire$SIZE_HA\n\n\nfires_no_geom &lt;- fires %&gt;% st_drop_geometry()\n\n#plot total area burned per year in BC, colored by fire cause\nburned_area_summary &lt;- fires_no_geom %&gt;% \n  group_by(FIRE_YEAR, FIRE_CAUSE) %&gt;% \n  summarize(total_burned_area = sum(SIZE_HA)) %&gt;%\n  ungroup()\n\nggplot(burned_area_summary, aes(x = FIRE_YEAR, y = total_burned_area, fill = FIRE_CAUSE)) +\n  geom_bar(stat = 'identity', position = 'stack') +\n  labs(\n    title = 'Total Area Burned per Year in BC based on Cause',\n    x = 'Year',\n    y = 'Total Area Burned (ha)',\n    fill = 'Fire Cause'\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: Total Area Burned per Year Based on Cause in BC\n\n\n\n\nStep 2 - Pre-processing Landsat 8 Data:\nSurface reflectance bands (1–7) are combined into multi-layer rasters, with cloud masking using QA_PIXEL values. Data is cropped to the Prouton Lakes fire perimeter. NDVI (Normalized Difference Vegetation Index) and NBR (Normalized Burn Ratio) are calculated. Visual Output: True color composite images before (July 7, 2015) and after the fire (July 15, 2018).\n\n\nCode\n#| execute: false\n#| eval: false\n#| code-fold: true\n#| code-summary: \"Show/Hide Code\"\n\n# set the base direcory where all the files are stored\nbase_directory &lt;- \"C:/Users/lorsigno.stu/OneDrive - UBC/Desktop/Documents/GEM 520/Lab 8/lab8_finalDemonstrationOfRSkills-assign/data/Landsat 8 OLI_TIRS C2 L2\"\n\n# set the output paths\nSR_dir &lt;- \"C:/Users/lorsigno.stu/OneDrive - UBC/Desktop/Documents/GEM 520/Lab 8/lab8_finalDemonstrationOfRSkills-assign/output/LC08_L2SP_048023_SR\"\nNDVI_dir &lt;- \"C:/Users/lorsigno.stu/OneDrive - UBC/Desktop/Documents/GEM 520/Lab 8/lab8_finalDemonstrationOfRSkills-assign/output/LC08_L2SP_048023_NDVI\"\nNBR_dir &lt;- \"C:/Users/lorsigno.stu/OneDrive - UBC/Desktop/Documents/GEM 520/Lab 8/lab8_finalDemonstrationOfRSkills-assign/output/LC08_L2SP_048023_NBR\"\n\n#listing all files within directory of Landsat tifs\n#flist &lt;- list.dirs(base_directory, full.names = TRUE, recursive = FALSE)\n\nfor (file in list.dirs(base_directory, full.names = TRUE, recursive = FALSE)) {\n  \n  # extract the product ID \n  tif_files &lt;- list.files(file, pattern = \".TIF$\", full.names = TRUE)\n  \n  #if (length(tif_files) &lt; 9) next\n  \n  product_ID &lt;- basename(file)\n  bands &lt;- tif_files[str_detect(basename(tif_files), \"SR_B[1-7]\")]\n  qa_pixel &lt;- tif_files[str_detect(basename(tif_files), \"QA_PIXEL\")]\n  \n  # extract raster bands and qa_pixel\n  sr_stack &lt;- rast(bands)\n  qa_pixel_stack &lt;- rast(qa_pixel)\n  \n  # mask clear conditions\n  sr_mask &lt;- mask(sr_stack, qa_pixel_stack == 21824)\n  \n  # change crs of prouton fire polygon shape from Albers to UTM10N\n  prouton_fire_utm &lt;- st_transform(prouton_fire, crs(sr_stack))\n  \n  # crop to the Proutons Lake Fire extent\n  sr_cropped &lt;- crop(sr_mask, vect(prouton_fire_utm))\n  \n  # calculate NDVI (NIR - red) / (NIR + red)\n  calc_ndvi &lt;- ((sr_cropped[[5]] - sr_cropped[[4]]) / (sr_cropped[[5]] + sr_cropped[[4]]))\n  \n  # calculate NBR (NIR - SWIR) / (NIR + SWIR)\n  calc_nbr &lt;- ((sr_cropped[[5]] - sr_cropped[[7]]) / (sr_cropped[[5]] + sr_cropped[[7]]))\n  \n  #sr_output &lt;- paste0(SR_dir, product_ID, \"_SR.tif\")\n  #ndvi_output &lt;- paste0(NDVI_dir, product_ID, \"_NDVI.tif\")\n  #nbr_output &lt;- paste0(NBR_dir, product_ID, \"_NBR.tif\")\n  \n  #writeRaster(sr_cropped, sr_output, overwrite = TRUE)\n  #writeRaster(calc_ndvi, ndvi_output, overwrite = TRUE)\n  #writeRaster(calc_nbr, nbr_output, overwrite = TRUE)\n\n}\n\n# pre-fire is July 7 2015\npre_fire_RGB &lt;- rast(\"~/GEM 520/Lab 8/lab8_finalDemonstrationOfRSkills-assign/output/LC08_L2SP_048023_SRLC08_L2SP_048023_20150707_20200909_02_T1_SR.tif\")\n# lets crop it to the fire extent\npre_fire_RGB_cropped &lt;- crop(pre_fire_RGB, vect(prouton_fire_utm))\nplotRGB(pre_fire_RGB_cropped, r = 4, g = 3, b = 2, stretch = \"lin\", main = \"Pre-Fire (2015)\")\n\n\n\n\n\n\n\n\n\nCode\n# post-fire is July 15 2018\npost_fire_RGB &lt;- rast(\"~/GEM 520/Lab 8/lab8_finalDemonstrationOfRSkills-assign/output/LC08_L2SP_048023_SRLC08_L2SP_048023_20180715_20200831_02_T1_SR.tif\")\npost_fire_RGB_cropped &lt;- crop(post_fire_RGB, vect(prouton_fire_utm))\nplotRGB(post_fire_RGB_cropped, r = 4, g = 3, b = 2, stretch = \"lin\", main = \"Post-Fire (2018)\")\n\n\n\n\n\n\n\n\n\n \n\n\nStep 3 - Yearly NDVI and NBR Composites:\nCompute annual average NDVI and NBR for the burned area. Visual Output: Line plot of mean NDVI over time, showing post-fire recovery trends.\n\n\nCode\n#| execute: false\n#| eval: false\n#| code-fold: true\n#| code-summary: \"Show/Hide Code\"\n\n# list all the files and make sure they exist\nndvi_list &lt;- list.files(NDVI_dir, pattern = \".tif$\", full.names = TRUE) #flist\nnbr_list &lt;- list.files(NBR_dir, pattern = \".tif$\", full.names = TRUE)\n\nndvi_name &lt;- basename(ndvi_list) # fname\nnbr_name &lt;- basename(nbr_list)\n\n# lets extract the 'text' dates from the ndvi_ and nbr_names\nndvi_date &lt;- str_sub(ndvi_name, start = 39, end = 46)\nnbr_date &lt;- str_sub(nbr_name, start = 38, end = 45)\n\n# we have just the dates stored as a vector, lets convert them to dates\nts_ndvi &lt;- as_date(ndvi_date,\n                   format = \"%Y%m%d\") # ts_dates\nts_nbr &lt;- as_date(nbr_date,\n                   format = \"%Y%m%d\")\n\n# now that we have some dates, lets open the files into a SpatRaster\nndvi_ts &lt;- rast(ndvi_list)\nnbr_ts &lt;- rast(nbr_list)\n\n# initiate an empty list for storing the composites\nndvi_composites_list &lt;- list()\n\nndvi_target_years &lt;- unique(year(ts_ndvi))\n\n# lets get looping!\nfor (year in seq_along(ndvi_target_years)) {\n  \n  # find the target years\n  trg_layers &lt;- which(year(ts_ndvi) == ndvi_target_years[year])\n  \n  # subset the target layers\n  ndvi_trg &lt;- subset(ndvi_ts, trg_layers)\n  \n  # calculate yearly average NDVI\n  ndvi_trg_avg &lt;- app(ndvi_trg, fun = mean, na.rm = TRUE)\n  \n  # assign name based on the year\n  names(ndvi_trg_avg) &lt;- paste0(\"\", ndvi_target_years[year])\n  \n  # add them to the list\n  ndvi_composites_list[[as.character(ndvi_target_years[year])]] &lt;- ndvi_trg_avg\n  \n}\n\nndvi_composite &lt;- rast(ndvi_composites_list)\n\n# lets do the same thing with NBR\nnbr_composite_list &lt;- list()\n\nnbr_target_years &lt;- unique(year(ts_nbr))\n\n# lets get looping!\nfor (y in 1:length(nbr_target_years)) {\n  \n  # find the target years\n  trg_layers &lt;- which(year(ts_nbr) == nbr_target_years[y])\n  \n  # subset the target layers\n  nbr_trg &lt;- subset(nbr_ts, trg_layers)\n  \n  # calculate yearly average NDVI\n  nbr_trg_avg &lt;- app(nbr_trg, fun = mean, na.rm = TRUE)\n  \n  # assign name based on the year\n  names(nbr_trg_avg) &lt;- paste0(\"\", nbr_target_years[y])\n  \n  # add them to the list\n  nbr_composite_list[[y]] &lt;- nbr_trg_avg\n  \n}\nnbr_composite &lt;- rast(nbr_composite_list)\n\nndvi_df &lt;- as.data.frame(ndvi_composite, xy = FALSE, na.rm = TRUE)\n\naverage_ndvi &lt;- colMeans(ndvi_df, na.rm = TRUE)\n\nndvi_summary &lt;- data.frame(\n  Year = as.numeric(gsub(\"Year_\", \"\", names(ndvi_composite))),\n  Average_NDVI = average_ndvi\n)\n\nggplot(ndvi_summary, aes(x = Year, y = Average_NDVI)) +\n  geom_point() +\n  geom_line(group = 1) +\n  labs(\n    title = \"Average NDVI of the Prouton Lakes Fire\",\n    x = \"Year\",\n    y = \"Average NDVI\"\n  ) +\n  theme_minimal() +\n  scale_x_continuous(breaks = seq(min(ndvi_summary$Year),\n                                  max(ndvi_summary$Year),\n                                  by = 1))\n\n\n\n\n\n\n\n\n\n\n\n\nPlotted Average NDVI over the Year of the Landsape\n\n\n\n\nStep 4 - Burn Severity Classification:\ndNBR (Delta NBR) is calculated using pre-fire (2015) and post-fire (2018) imagery. Burn severity is categorized into unburned, low, medium, and high severity using threshold values. Visual Output: Burn severity map with classification overlay.\n\n\nCode\n#| execute: false\n#| eval: false\n#| warning: false\n#| message: false\n#| code-fold: true\n#| code-summary: \"Show/Hide Code\"\n\n# set the pre and post fire nbr rasters\npre_fire_nbr &lt;- nbr_composite[[\"2015\"]]\npost_fire_nbr &lt;- nbr_composite[[\"2018\"]]\n\n\n\n# calculate the dNBR\ndNBR &lt;- pre_fire_nbr - post_fire_nbr\n\n# define the classification\nreclass_df &lt;- matrix(c(-0.2, 0.15, 1,\n                       0.15, 0.25, 2,\n                       0.25, 0.3, 3,\n                       0.3, 1.0, 4), ncol = 3, byrow = TRUE)\n\n\n# reclassify dNBR into the severity classes\nburn_severity &lt;- classify(dNBR, reclass_df)\n\n# set the burn severity for the matrix ids 1:4\nlevels(burn_severity) &lt;- data.frame(\n  id = 1:4,\n  label = c(\"unburned\", \"low severity\", \"medium severity\", \"high severity\"))\n\n# plot burn severity with the Prouton Fire Extent with legend\nplot(burn_severity, col = c(\"darkgreen\", \"yellow\", \"orange\", \"red\"),\n     main = \"Burn Severity of the Prouton Lakes Fire\")\nplot(prouton_fire_utm, add = TRUE, border = \"black\", lwd = 2, col = NA)\n\n\n\n\n\n\n\n\n\nCode\n# would have been a good place to look for black morels...\n\n\n\n\n\nPost-fire Classified Landscape Based on Fire Severity\n\n\n\n\nStep 5 - Post-Fire Vegetation Recovery:\nConvert burn severity raster into polygons. Extract NDVI values for each burn severity class from 2018–2021. Visual Outputs: - Yearly NDVI maps from 2018 to 2021 (same value range 0–0.5). - Boxplots showing NDVI distribution by burn severity class over time.\n\n\nCode\n#| execute: false\n#| eval: false\n#| code-fold: true\n#| code-summary: \"Show/Hide Code\"\n\n# first we need to convert the burn severity rasters to polygons\n# then load in the NDVI rasters from 2018, 2019, 2020 and 2021\n# and then extract the NDVI values by Burn Severity Class\n# then we can plot the yearly NDVI composites\n# finally create some boxplots showing the distribution of NDVI values per burn class by year\n\nburn_severity_masked &lt;- mask(burn_severity, vect(prouton_fire_utm)) # Mask to the fire extent\nburn_severity_polygons &lt;- as.polygons(burn_severity_masked, dissolve = TRUE)\n\nburn_severity_polygons$ID &lt;- 1:nrow(burn_severity_polygons)\n\nyears &lt;- c(2018, 2019, 2020, 2021)\nndvi_values &lt;- list()\n\nfor (year in years) {\n  ndvi_raster &lt;- ndvi_composites_list[[as.character(year)]]\n  \n  extracted_values &lt;- terra::extract(ndvi_raster, burn_severity_polygons,\n                                     xy = TRUE, ID = TRUE)\n  \n  extracted_values &lt;- merge(extracted_values, burn_severity_polygons, by = \"ID\",\n                            all.x = TRUE)\n  \n  colnames(extracted_values)[which(colnames(extracted_values) == as.character(year))] &lt;- \"value\"\n  \n  extracted_values$year &lt;- year\n  ndvi_values[[as.character(year)]] &lt;- extracted_values\n}\n\nndvi_all_years &lt;- do.call(rbind, ndvi_values)\n\nndvi_all_years$severity &lt;- factor(ndvi_all_years$label,\n                                  levels = c('unburned',\n                                             'low severity',\n                                             'medium severity',\n                                             'high severity'),\n                                  ordered = TRUE)\n\n\nfor (year in years) {\n  if (year %in% names(ndvi_composites_list)) {\n    plot(ndvi_composites_list[[as.character(year)]],\n         main = paste(\"Yearly NDVI Composite -\", year),\n         range = c(0, 0.5))\n    }\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(ndvi_all_years, aes(x = severity, y = value, fill = severity)) +\n  geom_boxplot() +\n  facet_wrap(~ year) +\n  scale_fill_manual(values = c(\"unburned\" = \"green\",\n                               \"low severity\" = \"yellow\",\n                               \"medium severity\" = \"orange\",\n                               \"high severity\" = \"red\"),\n                    name = \"Burn Severity\") +\n  labs(title = \"Distribution of NDVI Values by Burn Severity Class\",\n       x = \"Burn Severity Class\",\n       y = \"NDVI\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\n\n\n\nDistribution of NDVI Values by Burn Severity Class"
  }
]